{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 预测下一个汉字。 \n",
    "# 1. one word in, one word out \n",
    "# 2. mulitple word in, one word out.\n",
    "# 通过model的性能可以看出来，two word in 优于 one word in\n",
    "# 光使用10000条公交站数据，one word能到60%，two word能到66%。使用普通数据10000条，one word 26%, two word 34%，\n",
    "# 说明ngram确实是work的，另外，提高epoch和增加训练数据可以提升。\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from tensorflow.python.keras import backend as k\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Embedding\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "def find_top_n_in_prob_array(prob_array, topn=5):\n",
    "    idx = (-prob_array).argsort()[:topn]\n",
    "    return idx\n",
    "    \n",
    "def map_top_n_int_to_word(index_array, tokenizer, y_distribution):\n",
    "    int_set = set(index_array)\n",
    "    result = []\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index in int_set:\n",
    "            result.append((word, y_distribution[index]))\n",
    "    return result\n",
    "\n",
    "def find_topn_prob_word(y_distribution, model, tokenizer):\n",
    "    prob_array = y_distribution[0]\n",
    "    idx = find_top_n_in_prob_array(prob_array)\n",
    "    # print(idx)\n",
    "    topn_word = map_top_n_int_to_word(idx, tokenizer, prob_array)\n",
    "    topn_word.sort(key=lambda x: x[1], reverse=True)\n",
    "    return topn_word\n",
    "\n",
    "# 还输出各个字的预测概率\n",
    "def generate_one_word_with_prob(model, tokenizer, word, length=1):\n",
    "    in_text = word\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = encoded[-length:]\n",
    "    encoded = [encoded]\n",
    "    y_distribution = model.predict(encoded, verbose=0)\n",
    "    # predict_proba? 怎么返回top n个预测？\n",
    "    # print(y_distribution)\n",
    "    print(\"input {}\".format(word))\n",
    "    topn_word = find_topn_prob_word(y_distribution, model, tokenizer)\n",
    "    print(topn_word)\n",
    " \n",
    "def map_yaht_to_word(yhat, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == yhat:\n",
    "            return word\n",
    "    return ''\n",
    "\n",
    "# 实际上可以连续预测多个词\n",
    "# 实际中预测的后面的词就不太靠谱了 \n",
    "def generate_seq_with_one_word(model, tokenizer, seed_text, n_words):\n",
    "    in_text, result = seed_text, seed_text\n",
    "        # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        y_distribution = model.predict(encoded, verbose=0)\n",
    "        # predict_proba? 怎么返回top n个预测？\n",
    "        # print(y_distribution)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result\n",
    "\n",
    "# 实际上可以连续预测多个词\n",
    "def generate_seq_with_multiple_word(model, tokenizer, seed_text, n_words, length):\n",
    "    in_text, result = seed_text, seed_text\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = encoded[-length:]\n",
    "    seed_text_length = len(encoded)\n",
    "    encoded = [encoded]\n",
    "    if (seed_text_length < length):\n",
    "        print(\"context length is smaller than required length\")\n",
    "        return\n",
    "    for x in range(n_words):\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # 输出的class映射回word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "        # 改变输入变量\n",
    "        encoded[0].pop(0)\n",
    "        new_encode = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded[0].extend(new_encode)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 将中文用空格分开并返回，作为fit_on_text的输入\n",
    "def load_chinese(file, line_num=100000):\n",
    "    sentence_array = []\n",
    "    line_index = 0\n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            line_index += 1\n",
    "            line = line.strip()\n",
    "            current = []\n",
    "            for i in line:\n",
    "                if i != \"-\":\n",
    "                    current.append(i)\n",
    "            seg_str = \" \".join(current)\n",
    "            sentence_array.append(seg_str)\n",
    "            if line_index == line_num:\n",
    "                break\n",
    "    return sentence_array\n",
    "\n",
    "# 将空格分开的中文encode成int，作为神经网络的输入\n",
    "def encode_to_int(chinese_poi_name):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(chinese_poi_name)\n",
    "    encoded = tokenizer.texts_to_sequences(chinese_poi_name)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return tokenizer, encoded, vocab_size\n",
    "\n",
    "# 构造one-word-in, one-word-out的神经网络的输入\n",
    "def generate_word_pair(encoded):\n",
    "    sequences = list()\n",
    "    for i in encoded:\n",
    "        tuple_temp = tuple(i)\n",
    "        for pair in list(zip(i, i[1:])):\n",
    "            sequences.append(pair)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# 构造多个word的神经网络输入\n",
    "# encoded = [[1,2,3], [4,5,6]] length = 2\n",
    "# [1,2] [2,3] [4,5] [5,6]\n",
    "def generate_multiple_word_seq(encoded, length=1):\n",
    "    sequences = list()\n",
    "    for array in encoded:\n",
    "        # [95, 4, 233, 37, 2, 3, 1]\n",
    "        for idx, val in enumerate(array):\n",
    "            seq = []\n",
    "            if idx + length < len(array):\n",
    "                seq = array[idx:idx + length + 1]\n",
    "                sequences.append(seq) \n",
    "            else:\n",
    "                break\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences    \n",
    "\n",
    "# 生成X = [word_int1, word_int2, ...] -> word_int\n",
    "def gen_X_y(encoded, length=1):\n",
    "    word_seq = generate_multiple_word_seq(encoded, length)\n",
    "    sequences = array(word_seq)\n",
    "    X, y = sequences[:,0:-1],sequences[:,-1]\n",
    "    # 为什么是one hot的方式\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    return X, y\n",
    "\n",
    "def build_and_train_model(vocab_size, length, X, y):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=length))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=20, verbose=1)\n",
    "    return model\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9 2 8 台 湾 小 吃', '广 汇 医 药 ( 塔 西 路 )']\n",
      "[[124, 38, 135, 298, 171, 11, 322], [82, 243, 169, 122, 612, 15, 16]]\n",
      "vocab_size : 2578\n",
      "9 2 8 台 湾 小 吃\n",
      "[[124, 38, 135, 298, 171, 11, 322]]\n"
     ]
    }
   ],
   "source": [
    "# 预处理是一样的\n",
    "chinese_poi_name = load_chinese(\"./input_data/test_10000\")\n",
    "print(chinese_poi_name[0:2])\n",
    "tokenizer, encoded, vocab_size = encode_to_int(chinese_poi_name)\n",
    "print(encoded[0:2])\n",
    "print(\"vocab_size : {}\".format(vocab_size))\n",
    "print(chinese_poi_name[0])\n",
    "print(tokenizer.texts_to_sequences([chinese_poi_name[0]]))\n",
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 61459\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             25780     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2578)              131478    \n",
      "=================================================================\n",
      "Total params: 169,458\n",
      "Trainable params: 169,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 61459 samples\n",
      "Epoch 1/20\n",
      "61459/61459 [==============================] - 34s 548us/sample - loss: 6.4775 - accuracy: 0.0208\n",
      "Epoch 2/20\n",
      "61459/61459 [==============================] - 34s 547us/sample - loss: 6.1733 - accuracy: 0.0430\n",
      "Epoch 3/20\n",
      "61459/61459 [==============================] - 37s 603us/sample - loss: 5.7598 - accuracy: 0.1019\n",
      "Epoch 4/20\n",
      "61459/61459 [==============================] - 37s 600us/sample - loss: 5.3656 - accuracy: 0.1587\n",
      "Epoch 5/20\n",
      "61459/61459 [==============================] - 41s 668us/sample - loss: 5.1125 - accuracy: 0.1956\n",
      "Epoch 6/20\n",
      "61459/61459 [==============================] - 38s 619us/sample - loss: 4.9287 - accuracy: 0.2176\n",
      "Epoch 7/20\n",
      "61459/61459 [==============================] - 32s 527us/sample - loss: 4.7908 - accuracy: 0.2316\n",
      "Epoch 8/20\n",
      "61459/61459 [==============================] - 31s 505us/sample - loss: 4.6802 - accuracy: 0.2414\n",
      "Epoch 9/20\n",
      "61459/61459 [==============================] - 31s 508us/sample - loss: 4.5897 - accuracy: 0.2484\n",
      "Epoch 10/20\n",
      "61459/61459 [==============================] - 31s 510us/sample - loss: 4.5141 - accuracy: 0.2547\n",
      "Epoch 11/20\n",
      "61459/61459 [==============================] - 31s 509us/sample - loss: 4.4494 - accuracy: 0.2580\n",
      "Epoch 12/20\n",
      "61459/61459 [==============================] - 32s 516us/sample - loss: 4.3933 - accuracy: 0.2632\n",
      "Epoch 13/20\n",
      "61459/61459 [==============================] - 33s 540us/sample - loss: 4.3417 - accuracy: 0.2657\n",
      "Epoch 14/20\n",
      "61459/61459 [==============================] - 33s 540us/sample - loss: 4.2955 - accuracy: 0.2692\n",
      "Epoch 15/20\n",
      "61459/61459 [==============================] - 33s 533us/sample - loss: 4.2534 - accuracy: 0.2706\n",
      "Epoch 16/20\n",
      "61459/61459 [==============================] - 33s 533us/sample - loss: 4.2146 - accuracy: 0.2729\n",
      "Epoch 17/20\n",
      "61459/61459 [==============================] - 33s 532us/sample - loss: 4.1784 - accuracy: 0.2752\n",
      "Epoch 18/20\n",
      "61459/61459 [==============================] - 37s 604us/sample - loss: 4.1453 - accuracy: 0.2769\n",
      "Epoch 19/20\n",
      "61459/61459 [==============================] - 34s 558us/sample - loss: 4.1138 - accuracy: 0.2776\n",
      "Epoch 20/20\n",
      "61459/61459 [==============================] - 34s 560us/sample - loss: 4.0844 - accuracy: 0.2795\n",
      "input 公\n",
      "[('司', 0.7800371), ('寓', 0.043761324), ('厕', 0.033157058), ('室', 0.024029067), ('安', 0.020719672)]\n",
      "input 机\n",
      "[('械', 0.20761491), ('电', 0.16036084), ('构', 0.043845333), ('有', 0.042551123), ('专', 0.038712017)]\n",
      "input 阳\n",
      "[('光', 0.18493848), ('市', 0.055330098), ('能', 0.038582362), ('店', 0.030581884), ('县', 0.02329444)]\n",
      "公\n",
      "公 司\n",
      "机\n",
      "机 械\n",
      "阳\n",
      "阳 光\n"
     ]
    }
   ],
   "source": [
    "length_one = 1\n",
    "X1, y1 = gen_X_y(encoded, length_one)\n",
    "model1 = build_and_train_model(vocab_size, length_one, X1, y1)\n",
    "\n",
    "generate_one_word_with_prob(model1, tokenizer, \"公\")\n",
    "generate_one_word_with_prob(model1, tokenizer, \"机\")\n",
    "generate_one_word_with_prob(model1, tokenizer, \"阳\")\n",
    "\n",
    "# 测试效果 one word in\n",
    "test_word1 = []\n",
    "test_word1.append(\"公\")\n",
    "test_word1.append(\"机\")\n",
    "test_word1.append(\"阳\")\n",
    "for word in test_word1:\n",
    "    print(word)\n",
    "    print(generate_seq_with_one_word(model1, tokenizer, word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 51461\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 10)             25780     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2578)              131478    \n",
      "=================================================================\n",
      "Total params: 169,458\n",
      "Trainable params: 169,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 51461 samples\n",
      "Epoch 1/20\n",
      "51461/51461 [==============================] - 38s 736us/sample - loss: 6.3234 - accuracy: 0.0267\n",
      "Epoch 2/20\n",
      "51461/51461 [==============================] - 34s 651us/sample - loss: 5.9232 - accuracy: 0.0634\n",
      "Epoch 3/20\n",
      "51461/51461 [==============================] - 31s 596us/sample - loss: 5.6347 - accuracy: 0.1001\n",
      "Epoch 4/20\n",
      "51461/51461 [==============================] - 31s 593us/sample - loss: 5.3010 - accuracy: 0.1418\n",
      "Epoch 5/20\n",
      "51461/51461 [==============================] - 32s 613us/sample - loss: 4.9777 - accuracy: 0.1845\n",
      "Epoch 6/20\n",
      "51461/51461 [==============================] - 31s 605us/sample - loss: 4.7385 - accuracy: 0.2163\n",
      "Epoch 7/20\n",
      "51461/51461 [==============================] - 35s 685us/sample - loss: 4.5551 - accuracy: 0.2417\n",
      "Epoch 8/20\n",
      "51461/51461 [==============================] - 41s 801us/sample - loss: 4.4116 - accuracy: 0.2600\n",
      "Epoch 9/20\n",
      "51461/51461 [==============================] - 44s 863us/sample - loss: 4.2905 - accuracy: 0.2722\n",
      "Epoch 10/20\n",
      "51461/51461 [==============================] - 40s 768us/sample - loss: 4.1887 - accuracy: 0.2836\n",
      "Epoch 11/20\n",
      "51461/51461 [==============================] - 39s 764us/sample - loss: 4.1010 - accuracy: 0.2917\n",
      "Epoch 12/20\n",
      "51461/51461 [==============================] - 43s 840us/sample - loss: 4.0225 - accuracy: 0.3018\n",
      "Epoch 13/20\n",
      "51461/51461 [==============================] - 44s 863us/sample - loss: 3.9519 - accuracy: 0.3097\n",
      "Epoch 14/20\n",
      "51461/51461 [==============================] - 40s 786us/sample - loss: 3.8872 - accuracy: 0.3155\n",
      "Epoch 15/20\n",
      "51461/51461 [==============================] - 38s 743us/sample - loss: 3.8288 - accuracy: 0.3230\n",
      "Epoch 16/20\n",
      "51461/51461 [==============================] - 36s 705us/sample - loss: 3.7739 - accuracy: 0.3271\n",
      "Epoch 17/20\n",
      "51461/51461 [==============================] - 47s 917us/sample - loss: 3.7232 - accuracy: 0.3340\n",
      "Epoch 18/20\n",
      "51461/51461 [==============================] - 49s 956us/sample - loss: 3.6761 - accuracy: 0.3373\n",
      "Epoch 19/20\n",
      "51461/51461 [==============================] - 43s 829us/sample - loss: 3.6312 - accuracy: 0.3426\n",
      "Epoch 20/20\n",
      "51461/51461 [==============================] - 38s 735us/sample - loss: 3.5894 - accuracy: 0.3459\n"
     ]
    }
   ],
   "source": [
    "length_two = 2\n",
    "X2, y2 = gen_X_y(encoded, length_two)\n",
    "model2 = build_and_train_model(vocab_size, length_two, X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 32]]\n",
      "input 公 安\n",
      "[('局', 0.49886608), ('店', 0.03898837), ('处', 0.03244424), ('大', 0.021874331), ('队', 0.02081065)]\n",
      "input 机 关\n",
      "[('馆', 0.017227916), ('大', 0.0138319), ('龙', 0.01355679), ('新', 0.013397972), ('n', 0.012464523)]\n",
      "input 阳 光\n",
      "[('路', 0.059749976), ('花', 0.044486042), ('苑', 0.020730225), ('商', 0.018756453), ('明', 0.017591927)]\n",
      "公 安\n",
      "公 安 局\n",
      "机 关\n",
      "机 关 馆\n",
      "阳 光\n",
      "阳 光 路\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences([\"公 安\"]))\n",
    "\n",
    "generate_one_word_with_prob(model2, tokenizer, \"公 安\", 2)\n",
    "generate_one_word_with_prob(model2, tokenizer, \"机 关\", 2)\n",
    "generate_one_word_with_prob(model2, tokenizer, \"阳 光\", 2)\n",
    "\n",
    "\n",
    "# 测试效果 multiple word in \n",
    "test_word2 = []\n",
    "test_word2.append(\"公 安\")\n",
    "test_word2.append(\"机 关\")\n",
    "test_word2.append(\"阳 光\")\n",
    "\n",
    "for word in test_word2:\n",
    "    print(word)\n",
    "    print(generate_seq_with_multiple_word(model2, tokenizer, word, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1, 10)             25780     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2578)              131478    \n",
      "=================================================================\n",
      "Total params: 169,458\n",
      "Trainable params: 169,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_details(model):\n",
    "    model.summary()\n",
    "    model.get_config()\n",
    "    \n",
    "def model_layer_information(model, layer_index):\n",
    "    print(model.layers[layer_index].input_shape)\n",
    "    print(model.layers[layer_index].output_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
