{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "from tensorflow.python.keras import backend as k\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Embedding\n",
    "\n",
    "import itertools\n",
    " \n",
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "    in_text, result = seed_text, seed_text\n",
    "        # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chinese(file):\n",
    "    sentence_array = []\n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            current = []\n",
    "            for i in line:\n",
    "                if i != \"-\":\n",
    "                    current.append(i)\n",
    "            seg_str = \" \".join(current)\n",
    "            sentence_array.append(seg_str)\n",
    "    return sentence_array\n",
    "\n",
    "chinese_poi_name = load_chinese(\"./input_data/poi_1000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_poi_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(chinese_poi_name)\n",
    "# tokenizer.word_index\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts([\"Jack and Jill went up the hill\"])\n",
    "# print(tokenizer.word_index)\n",
    "# tokenizer.texts_to_sequences([\"jack\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chinese_poi_name[0])\n",
    "print(type(chinese_poi_name[0]))\n",
    "print(tokenizer.word_index[\"é“\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences([chinese_poi_name[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences(chinese_poi_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# create word -> word sequences\n",
    "def generate_word_pair(encoded):\n",
    "    sequences = list()\n",
    "    for i in encoded:\n",
    "        tuple_temp = tuple(i)\n",
    "        for pair in list(zip(i, i[1:])):\n",
    "            sequences.append(pair)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "chinese_word_pair = generate_word_pair(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(chinese_word_pair))\n",
    "print(chinese_word_pair[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tuple([338, 15, 101, 112, 2, 24, 20])\n",
    "print(type(test))\n",
    "print(list(zip(test, test[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into X and y elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,0],sequences[:,1]\n",
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(generate_seq(model, tokenizer, 'Jack', 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
