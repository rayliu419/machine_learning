{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据当前的词，预测下一个词。 \n",
    "# 1. one word in, one word out\n",
    "# 2. mulitple word in, multiple word out.\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from tensorflow.python.keras import backend as k\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Embedding\n",
    "\n",
    "import itertools\n",
    " \n",
    "# 实际上可以连续预测多个词\n",
    "def generate_seq_with_one_word(model, tokenizer, seed_text, n_words):\n",
    "    in_text, result = seed_text, seed_text\n",
    "        # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result\n",
    "\n",
    "# 实际上可以连续预测多个词\n",
    "def generate_seq_with_multiple_word(model, tokenizer, seed_text, n_words, length):\n",
    "    in_text, result = seed_text, seed_text\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[-length:]\n",
    "    encoded = array(encoded)\n",
    "    if (len(encode_init) < length):\n",
    "        print(\"context length is smaller than required length\")\n",
    "        return\n",
    "    for _ in range(n_words):\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # 输出的class映射回word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "        # 改变输入变量\n",
    "        encoded.pop(0)\n",
    "        new_encode = tokenizer.texts_to_sequences([in_text])\n",
    "        encoded.append(new_encode)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 将中文用空格分开并返回，作为fit_on_text的输入\n",
    "def load_chinese(file, line_num=100000):\n",
    "    sentence_array = []\n",
    "    line_index = 0\n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "            line_index += 1\n",
    "            line = line.strip()\n",
    "            current = []\n",
    "            for i in line:\n",
    "                if i != \"-\":\n",
    "                    current.append(i)\n",
    "            seg_str = \" \".join(current)\n",
    "            sentence_array.append(seg_str)\n",
    "            if line_index == line_num:\n",
    "                break\n",
    "    return sentence_array\n",
    "\n",
    "# 将空格分开的中文encode成int，作为神经网络的输入\n",
    "def encode_to_int(chinese_poi_name):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(chinese_poi_name)\n",
    "    encoded = tokenizer.texts_to_sequences(chinese_poi_name)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return tokenizer, encoded, vocab_size\n",
    "\n",
    "# 构造one-word-in, one-word-out的神经网络的输入\n",
    "def generate_word_pair(encoded):\n",
    "    sequences = list()\n",
    "    for i in encoded:\n",
    "        tuple_temp = tuple(i)\n",
    "        for pair in list(zip(i, i[1:])):\n",
    "            sequences.append(pair)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# 构造多个word的神经网络输入\n",
    "# encoded = [[1,2,3], [4,5,6]] length = 2\n",
    "# [1,2] [2,3] [4,5] [5,6]\n",
    "def generate_multiple_word_seq(encoded, length=1):\n",
    "    sequences = list()\n",
    "    for array in encoded:\n",
    "        for idx, val in enumerate(array):\n",
    "            seq = []\n",
    "            if idx + length <= len(array):\n",
    "                seq = array[idx:idx + length]\n",
    "                sequences.append(seq) \n",
    "            else:\n",
    "                break\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences    \n",
    "\n",
    "# 生成X = [word_int1, word_int2, ...] -> word_int\n",
    "def gen_X_y(encoded, length=1):\n",
    "    word_seq = generate_multiple_word_seq(encoded, length)\n",
    "    print(word_seq[0:10])\n",
    "    sequences = array(word_seq)\n",
    "    X, y = sequences[:,0:-2],sequences[:,-1]\n",
    "    # 为什么是one hot的方式\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    print(y[0:5])\n",
    "    return X, y\n",
    "\n",
    "def build_and_train_model(vocab_size, length=1, X, y)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=length))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=50, verbose=1)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理是一样的\n",
    "chinese_poi_name = load_chinese(\"./input_data/poi_1000000\", 10000)\n",
    "print(chinese_poi_name[0:2])\n",
    "tokenizer, encoded, vocab_size = encode_to_int(chinese_poi_name)\n",
    "print(encoded[0:2])\n",
    "print(\"vocab_size : {}\".format(vocab_size))\n",
    "print(chinese_poi_name[0])\n",
    "print(type(chinese_poi_name[0]))\n",
    "print(tokenizer.word_index[\"铁\"])\n",
    "print(tokenizer.texts_to_sequences([chinese_poi_name[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index\n",
    "\n",
    "# 测试效果 one word in\n",
    "test_word1 = []\n",
    "test_word1.append(\"公\")\n",
    "test_word1.append(\"机\")\n",
    "test_word1.append(\"阳\")\n",
    "\n",
    "length_one = 1\n",
    "X1, y1 = gen_X_y(encoded, length_one)\n",
    "model1 = build_and_train_model(vocab_size, length_one, X1, y1)\n",
    "for word in test_word:\n",
    "    print(generate_seq_with_one_word(model, tokenizer, word, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 测试效果 multiple word in \n",
    "test_word2 = []\n",
    "test_word2.append(\"公 交\")\n",
    "test_word2.append(\"机 关\")\n",
    "test_word2.append(\"阳 光\")\n",
    "\n",
    "length_two = 2\n",
    "X2, y2 = gen_X_y(encoded, length_two)\n",
    "model2 = build_and_train_model(vocab_size, length_one, X2, y2)\n",
    "for word in test_word2:\n",
    "    print(generate_seq_with_multiple_word(model, tokenizer, word, 1， 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
