{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "实现一个非常简单的relu神经网络。\n",
    "原始的numpy版本会很快发生梯度爆炸或者梯度消失，改变learning rate有一定的帮助。原始版本主要是注意怎么求导的问题。\n",
    "梯度爆炸这里表现出来self.w1/self.w2的值很大，所以应该也可以通过修改loss函数加入正则项来修正。\n",
    "但是使用torch的版本，不仅收敛快，而且不会发生梯度消失/爆炸，估计是optimizer和初始化方面的方面。\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimpleReLUNN():\n",
    "    def __init__(self, x, y, sample_num, sample_feature, hidden_units, hidden_output_dim, lr):\n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        # 神经网络的权重\n",
    "        # 每个hidden unit有一个vector对应feature, 1000个feature, 1000个权重，每个unit 1000个权重\n",
    "        self.w1 = np.random.randn(sample_feature, hidden_units)\n",
    "        # 与上面类似。\n",
    "        self.w2 = np.random.randn(hidden_units, hidden_output_dim)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        print(\"x.shape\\n\", x.shape)\n",
    "        print(\"y.shape\\n\", y.shape)\n",
    "        print(\"w1.shape\\n\", self.w1.shape)\n",
    "        print(\"w2.shape\\n\", self.w2.shape)\n",
    "        # y_predict = 0\n",
    "        self.y_predict = np.zeros((sample_num, hidden_output_dim), dtype=np.float)\n",
    "\n",
    "    \"\"\"\n",
    "    前向传播 \n",
    "    \"\"\"\n",
    "    def forward(self):\n",
    "        # 1d数组内积，2d数学矩阵乘法，注意不一样。\n",
    "        # hidden unit的结果\n",
    "        h = self.x.dot(self.w1)\n",
    "        # h.shape (64,100)\n",
    "        # element-wise maximum，可以直接作用到2d情况\n",
    "        relu_output = np.maximum(h, 0)\n",
    "        # relu_output.shape (64, 100)\n",
    "        y_predict = relu_output.dot(self.w2)\n",
    "        # y_predict.shape (64, 100)\n",
    "        self.y_predict = y_predict\n",
    "\n",
    "    \"\"\"\n",
    "    loss function - MSE\n",
    "    BGD - 在更新参数时使用所有的样本来进行更新。\n",
    "    \"\"\"\n",
    "    def loss(self):\n",
    "        y_diff = self.y_predict - self.y\n",
    "        # shape = (64, 100)\n",
    "        y_diff_square = np.square(y_diff)\n",
    "        # shape - ?\n",
    "        loss = y_diff_square.sum()\n",
    "        return loss\n",
    "\n",
    "    \"\"\"\n",
    "    反向传播函数，更新w1, w2\n",
    "    实际上这里的loss函数是\n",
    "    loss = (y - w2 * relu(w1 * x))^2，要想这个尽量小，loss对w2, w1求导。\n",
    "    loss对w2求导：\n",
    "    2(y - w2 * relu(w1 * x)) * relu(w1 * x), 而w2 * relu(w1 * x)其实就是y_predict，前面已经结算完了。\n",
    "    \"\"\"\n",
    "    def back_propagation(self):\n",
    "        # (64, 100)\n",
    "        grad_y_predict = 2.0 * (self.y_predict - self.y)\n",
    "        # update w2\n",
    "        grad_w2 = np.maximum(self.x.dot(self.w1), 0).T.dot(grad_y_predict)\n",
    "        self.w2 = self.w2 - self.lr * grad_w2\n",
    "        # update w1\n",
    "        # (64, 100)\n",
    "        \"\"\"\n",
    "        这里其实是求复合函数relu(w1 * x)的导数\n",
    "        w1 * x < 0时，导数为0,\n",
    "        w1 * x > 0时，导数=x,\n",
    "        这里相当于是，如果当前权重w1 * x 的训练集中，过滤掉w1 * x < 0的权重变化。\n",
    "        \"\"\"\n",
    "        relu_inner = self.x.dot(self.w1)\n",
    "        # (64, 100)\n",
    "        grad_h = grad_y_predict.dot(self.w2.T)\n",
    "        # 这个操作相当于原始是一个2d矩阵，传入的也是相同形状的同一个2d矩阵，element-wise的操作，这个操作我不熟悉\n",
    "        grad_h[relu_inner < 0] = 0\n",
    "        # (1000, 100)\n",
    "        grad_w1 = self.x.T.dot(grad_h)\n",
    "        self.w1 = self.w1 - self.lr * grad_w1\n",
    "\n",
    "\n",
    "def run_model(model, iter):\n",
    "    print(\"lr - {}\".format(model.lr))\n",
    "    for i in range(iter):\n",
    "        model.forward()\n",
    "        print(i, model.loss())\n",
    "        model.back_propagation()\n",
    "\n",
    "sample_num = 64\n",
    "sample_feature = 100\n",
    "hidden_units = 10\n",
    "# 每个unit的输出维度？\n",
    "hidden_output_dim = 10\n",
    "iter = 50\n",
    "# 训练集 x y\n",
    "x = np.random.randn(sample_num, sample_feature)\n",
    "y = np.random.randn(sample_num, hidden_output_dim)\n",
    "\n",
    "model_lr1 = SimpleReLUNN(x, y, sample_num, sample_feature, hidden_units, hidden_output_dim, 1e-4)\n",
    "model_lr2 = SimpleReLUNN(x, y, sample_num, sample_feature, hidden_units, hidden_output_dim, 1e-5)\n",
    "model_lr3 = SimpleReLUNN(x, y, sample_num, sample_feature, hidden_units, hidden_output_dim, 1e-6)\n",
    "\n",
    "print(\"==================================================================\")\n",
    "run_model(model_lr1, iter)\n",
    "print(\"==================================================================\")\n",
    "run_model(model_lr2, iter)\n",
    "print(\"==================================================================\")\n",
    "run_model(model_lr3, iter)\n",
    "print(\"==================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sample_num = 64\n",
    "sample_feature = 100\n",
    "hidden_units = 10\n",
    "# 每个unit的输出维度？\n",
    "hidden_output_dim = 10\n",
    "iter = 50\n",
    "\n",
    "x = torch.randn(sample_num, sample_feature)\n",
    "y = torch.randn(sample_num, hidden_output_dim)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, sample_feature, hidden_units, hidden_output_dim):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # 定义模型计算图\n",
    "        self.linear1 = torch.nn.Linear(sample_feature, hidden_units, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(hidden_units, hidden_output_dim, bias=False)\n",
    "\n",
    "    # 在class中必须定于前向传播过程，torch.nn.Sequential则不需要\n",
    "    def forward(self, x):\n",
    "        # 两个模型相连的前向传播\n",
    "        # clamp实现relu\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "# model使用前面定义的类\n",
    "model2 = TwoLayerNet(sample_feature, hidden_units, hidden_output_dim)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(iter):\n",
    "    # 前向传播\n",
    "    y_pred = model2(x)           # model.forward() 和model(x)是一样的\n",
    "    # 计算损失\n",
    "    loss = loss_fn(y_pred, y)   # computation graph\n",
    "    print(it, loss.item())\n",
    "    # 梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 参数自动一步更新\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
