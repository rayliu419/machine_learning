{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "情感预测的练习。对于这个练习，\n",
    "对于精度有影响的：\n",
    "    1.增加训练集。5000->10000，精度提升10%。\n",
    "    2.增加模型复杂度。simple->LSTM，更重要的是simple容易过拟合。\n",
    "    3.stopword在此问题中不应该去除，会降低精度。主要原因应该是shouldn't之类可以判断情感的词被过滤掉了。\n",
    "    4.增加epoch的次数。\n",
    "减小过拟合的方法：\n",
    "    1.加入dropout。\n",
    "    2.降低模型复杂度。LSTM单元数减少。\n",
    "    3.增加训练集。\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import zeros\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Bidirectional, Input \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import regularizers\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "max_row = 10000\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type, file_name):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def correct_and_wrong():\n",
    "    # fit_on_texts错误的用法\n",
    "    tokenizer2 = Tokenizer(num_words=5000)\n",
    "    tokenizer2.fit_on_texts([[\"I am god\"], [\"you are idiot\"]])\n",
    "    print(tokenizer2.word_index)\n",
    "    # fit_on_texts正确的用法\n",
    "    tokenizer3 = Tokenizer(num_words=5000)\n",
    "    tokenizer3.fit_on_texts([[\"I\", \"am\", \"god\"], [\"you\", \"are\", \"idiot\"]])\n",
    "    print(tokenizer3.word_index)\n",
    "    # gensim.models.Word2Vec的错误用法\n",
    "    word_model2 = gensim.models.Word2Vec([[\"I am god\"], [\"you are idiot\"]], min_count=1, size=5, window=5)\n",
    "    word_model2.wv.save_word2vec_format(\"wrong_embedding\")\n",
    "    # gensim.models.Word2Vec的正确用法\n",
    "    word_model3 = gensim.models.Word2Vec([[\"I\", \"am\", \"god\"], [\"you\", \"are\", \"idiot\"]], min_count=1, size=5, window=5)\n",
    "    word_model3.wv.save_word2vec_format(\"correct_embedding\")\n",
    "\n",
    "\n",
    "def column_text_to_sentence_array(lines, use_stop_word=False):\n",
    "    stopwords_en = set()\n",
    "    if use_stop_word:\n",
    "        stopwords_en = set(stopwords.words('english'))\n",
    "    sentence_array = []\n",
    "    word_num = 0\n",
    "    word_set = set()\n",
    "    for line in lines:\n",
    "        temp = []\n",
    "        for word in word_tokenize(line):\n",
    "            if word not in stopwords_en:\n",
    "                word_num += 1\n",
    "                word_uniform = word.lower()\n",
    "                word_set.add(word_uniform)\n",
    "                temp.append(word_uniform)\n",
    "        sentence_array.append(temp)\n",
    "    uniq_word_num = len(word_set)\n",
    "    return sentence_array, word_num, word_set\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def predict_single(review, model):\n",
    "    review = tokenizer.texts_to_sequences(review)\n",
    "    flat_list = []\n",
    "    for sublist in review:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    flat_list = [flat_list]\n",
    "    review = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "    print(model.predict(review))\n",
    "\n",
    "\n",
    "def build_int_to_vector_mapping(tokenizer, word_vector):\n",
    "    \"\"\"\n",
    "    现在有两个映射\n",
    "    1. gensim 的 word->vector 存在word_vector\n",
    "    2. tokenizer 的 word->int 存在tokenizer\n",
    "    矩阵中存放 int->vector的映射\n",
    "    最终的映射是 [\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] -> [[0.1, 0.4], [0.21, 0.233] ...]\n",
    "    在Embedding layer embedding matrix被传入\n",
    "    \"\"\"\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vector[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "        except KeyError:\n",
    "            print(\"word {} is OOV\".format(word))\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# 1. 读取数据\n",
    "movie_reviews = pd.read_csv(\"./input_data/IMDB_Dataset.csv\")\n",
    "movie_reviews.info()\n",
    "# movie_reviews = movie_reviews[0:max_row]\n",
    "# 2. 预处理数据\n",
    "movie_reviews[\"review\"] = movie_reviews[\"review\"].map(lambda x: preprocess_text(x))\n",
    "movie_reviews[\"sentiment\"] = movie_reviews[\"sentiment\"].map(lambda x: 1 if x==\"positive\" else 0)\n",
    "movie_reviews.head()\n",
    "X = movie_reviews[\"review\"]\n",
    "Y = movie_reviews[\"sentiment\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "# 3. 使用gensim训练词向量\n",
    "data, word_num, word_set = column_text_to_sentence_array(movie_reviews[\"review\"], False)\n",
    "# cannot -> can, not\n",
    "# data[13]\n",
    "print(\"total word number : {}\".format(word_num))\n",
    "print(\"total uniq word : {}\".format(len(word_set)))\n",
    "model_COBW = gensim.models.Word2Vec(data, min_count=1, size=100, window=5)\n",
    "word_vector = model_COBW.wv\n",
    "# like word -> vector dict\n",
    "print(\"word to vecotr number : {}\".format(len(word_vector.index2word)))\n",
    "\n",
    "# 4. 将word数组转换为int数组，只保存5000个频率最高的映射？\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\"\"\"\n",
    "correct_and_wrong()\n",
    "tokenizer.fit_on_texts(X_train) 这里不能这么写，要用data来fit_on_texts。\n",
    "原因是nltk.tokenize的word_tokenize和keras.preprocessing.fit_on_texts对于有些词不一样，例如cannot, nltk处理成了can和not，但是\n",
    "fit_on_text的时候将 cannot当成一个词了\n",
    "\"\"\"\n",
    "print(\"word to int number : {}\".format(len(tokenizer.word_index)))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 填充成一样的长度\n",
    "# 如果越过就是不填充\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 获取输入层的weights\n",
    "embedding_mapping = build_int_to_vector_mapping(tokenizer, word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 训练和测试\n",
    "sample_review = X[57]\n",
    "# print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    return dt_string\n",
    "\n",
    "def epoch_performance(history, model_name):\n",
    "    print(\"model name : {}\".format(model_name))\n",
    "    for i in history.epoch:\n",
    "        print(\"epoch {}\".format(i))\n",
    "        print(\"train acc : {} validate acc : {}\".format(history.history[\"acc\"][i], history.history[\"val_acc\"][i]))\n",
    "        print(\"train loss : {} validate loss : {}\".format(history.history[\"loss\"][i], history.history[\"val_loss\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model = Sequential()\n",
    "    \"\"\"\n",
    "    All that the Embedding layer does is to map the integer inputs to the vectors found at the\n",
    "    corresponding index in the embedding matrix,\n",
    "    i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "    这种模型overfitting比较严重。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    # 光通过这个解决不了overfitting的问题\n",
    "    # model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file , show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    epoch_performance(history, model_name)\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training simple model\")\n",
    "model1 = simple_model(\"simple_model\" ,vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_many_to_one_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model\n",
    "\n",
    "def LSTM_many_to_one_model_v2(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    降低模型的overfit。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    # 减少LSTM的个数\n",
    "    model.add(LSTM(32))\n",
    "    # 加入dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"start training LSTM v1 model\")\n",
    "# model3 = LSTM_many_to_one_model(\"LSTM_v1\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "print(\"start training LSTM v2 model\")\n",
    "model4 = LSTM_many_to_one_model_v2(\"LSTM_v2\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BI_LSTM_many_to_one_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    降低模型的overfit。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training BI_LSTM model\")\n",
    "model5 = BI_LSTM_many_to_one_model(\"BI_LSTM_v1\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_many_to_one_model_v3(model_name, dict_size, embedding_matrix, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    变长input，但是实际上通过不了。至少在tensorflow 1.15版本是不行的。\n",
    "    无论是Sequential()还是function API。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    \n",
    "    embedding_input = Input(shape=(None,))\n",
    "    embedding_output = Embedding(dict_size, 100, weights=[embedding_matrix], trainable=False, input_length=None)(embedding_input)\n",
    "    LSTM_output = LSTM(32)(embedding_output)\n",
    "    dropout_output = Dropout(0.5)(LSTM_output)\n",
    "    dense_output = Dense(1, activation='sigmoid')(dropout_output)\n",
    "    \n",
    "    model = Model(embedding_input, dense_output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "        \n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际上无法运行\n",
    "# print(\"start training LSTM v3 model\")\n",
    "# model6 = LSTM_many_to_one_model_v3(\"LSTM_v3\", vocab_size, embedding_mapping, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_callbacks(model_save_file):\n",
    "    \"\"\"\n",
    "    加入early stopping\n",
    "    降低learning rate\n",
    "    \"\"\"\n",
    "    # save model callback\n",
    "    checkpoint = ModelCheckpoint(model_save_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    # early stop callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "    # reduce learning rate\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1)\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "    \n",
    "    \n",
    "def LSTM_many_to_one_model_v4(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    当前最高的val_acc是86%左右，想办法继续优化LSTM_many_to_one_model_v2\n",
    "    1.尝试加入更多的dense+relu，两层dropout - done\n",
    "    2.dense加入egularizer。 - done\n",
    "    3.加入early stopping。 - done\n",
    "    4.降低learning rate。- done\n",
    "    5.两个LSTM?\n",
    "    \n",
    "    从实验的角度来看：\n",
    "        dense加入egularizer没什么用。\n",
    "        降低lr能小幅度提高。\n",
    "        两层dropout也没什么用。\n",
    "        early stopping只能节省时间。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model_save_file = model_info_file_prefix + \"save_model\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    # 减少LSTM的个数\n",
    "    model.add(LSTM(16))\n",
    "    # 加入dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10,  kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    \n",
    "    call_backs = generate_model_callbacks(model_save_file)\n",
    "    history_plot = LossHistory()\n",
    "    call_backs.append(history_plot)\n",
    "    \n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=call_backs)\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training LSTM v4 model\")\n",
    "model7 = LSTM_many_to_one_model_v4(\"LSTM_v4\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
