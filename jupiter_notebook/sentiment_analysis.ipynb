{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "情感预测的练习。对于这个练习，\n",
    "对于精度有影响的：\n",
    "    1.增加训练集。5000->10000，精度提升10%。\n",
    "    2.增加模型复杂度。simple->LSTM，更重要的是simple容易过拟合。\n",
    "    3.stopword在此问题中不应该去除，会降低精度。主要原因应该是shouldn't之类可以判断情感的词被过滤掉了。\n",
    "    4.增加epoch的次数。\n",
    "减小过拟合的方法：\n",
    "    1.加入dropout。\n",
    "    2.降低模型复杂度。LSTM单元数减少。\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import zeros\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Bidirectional, Input \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import regularizers\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "max_row = 10000\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type, file_name):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def correct_and_wrong():\n",
    "    # fit_on_texts错误的用法\n",
    "    tokenizer2 = Tokenizer(num_words=5000)\n",
    "    tokenizer2.fit_on_texts([[\"I am god\"], [\"you are idiot\"]])\n",
    "    print(tokenizer2.word_index)\n",
    "    # fit_on_texts正确的用法\n",
    "    tokenizer3 = Tokenizer(num_words=5000)\n",
    "    tokenizer3.fit_on_texts([[\"I\", \"am\", \"god\"], [\"you\", \"are\", \"idiot\"]])\n",
    "    print(tokenizer3.word_index)\n",
    "    # gensim.models.Word2Vec的错误用法\n",
    "    word_model2 = gensim.models.Word2Vec([[\"I am god\"], [\"you are idiot\"]], min_count=1, size=5, window=5)\n",
    "    word_model2.wv.save_word2vec_format(\"wrong_embedding\")\n",
    "    # gensim.models.Word2Vec的正确用法\n",
    "    word_model3 = gensim.models.Word2Vec([[\"I\", \"am\", \"god\"], [\"you\", \"are\", \"idiot\"]], min_count=1, size=5, window=5)\n",
    "    word_model3.wv.save_word2vec_format(\"correct_embedding\")\n",
    "\n",
    "\n",
    "def column_text_to_sentence_array(lines, use_stop_word=False):\n",
    "    stopwords_en = set()\n",
    "    if use_stop_word:\n",
    "        stopwords_en = set(stopwords.words('english'))\n",
    "    sentence_array = []\n",
    "    word_num = 0\n",
    "    word_set = set()\n",
    "    for line in lines:\n",
    "        temp = []\n",
    "        for word in word_tokenize(line):\n",
    "            if word not in stopwords_en:\n",
    "                word_num += 1\n",
    "                word_uniform = word.lower()\n",
    "                word_set.add(word_uniform)\n",
    "                temp.append(word_uniform)\n",
    "        sentence_array.append(temp)\n",
    "    uniq_word_num = len(word_set)\n",
    "    return sentence_array, word_num, word_set\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def predict_single(review, model):\n",
    "    review = tokenizer.texts_to_sequences(review)\n",
    "    flat_list = []\n",
    "    for sublist in review:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    flat_list = [flat_list]\n",
    "    review = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "    print(model.predict(review))\n",
    "\n",
    "\n",
    "def build_int_to_vector_mapping(tokenizer, word_vector):\n",
    "    \"\"\"\n",
    "    现在有两个映射\n",
    "    1. gensim 的 word->vector 存在word_vector\n",
    "    2. tokenizer 的 word->int 存在tokenizer\n",
    "    矩阵中存放 int->vector的映射\n",
    "    最终的映射是 [\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] -> [[0.1, 0.4], [0.21, 0.233] ...]\n",
    "    在Embedding layer embedding matrix被传入\n",
    "    \"\"\"\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vector[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "        except KeyError:\n",
    "            print(\"word {} is OOV\".format(word))\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# 1. 读取数据\n",
    "movie_reviews = pd.read_csv(\"./input_data/IMDB_Dataset.csv\")\n",
    "movie_reviews.info()\n",
    "movie_reviews = movie_reviews[0:max_row]\n",
    "# 2. 预处理数据\n",
    "movie_reviews[\"review\"] = movie_reviews[\"review\"].map(lambda x: preprocess_text(x))\n",
    "movie_reviews[\"sentiment\"] = movie_reviews[\"sentiment\"].map(lambda x: 1 if x==\"positive\" else 0)\n",
    "movie_reviews.head()\n",
    "X = movie_reviews[\"review\"]\n",
    "Y = movie_reviews[\"sentiment\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "# 3. 使用gensim训练词向量\n",
    "data, word_num, word_set = column_text_to_sentence_array(movie_reviews[\"review\"], False)\n",
    "# cannot -> can, not\n",
    "# data[13]\n",
    "print(\"total word number : {}\".format(word_num))\n",
    "print(\"total uniq word : {}\".format(len(word_set)))\n",
    "model_COBW = gensim.models.Word2Vec(data, min_count=1, size=100, window=5)\n",
    "word_vector = model_COBW.wv\n",
    "# like word -> vector dict\n",
    "print(\"word to vecotr number : {}\".format(len(word_vector.index2word)))\n",
    "\n",
    "# 4. 将word数组转换为int数组，只保存5000个频率最高的映射？\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\"\"\"\n",
    "correct_and_wrong()\n",
    "tokenizer.fit_on_texts(X_train) 这里不能这么写，要用data来fit_on_texts。\n",
    "原因是nltk.tokenize的word_tokenize和keras.preprocessing.fit_on_texts对于有些词不一样，例如cannot, nltk处理成了can和not，但是\n",
    "fit_on_text的时候将 cannot当成一个词了\n",
    "\"\"\"\n",
    "print(\"word to int number : {}\".format(len(tokenizer.word_index)))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 填充成一样的长度\n",
    "# 如果越过就是不填充\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 获取输入层的weights\n",
    "embedding_mapping = build_int_to_vector_mapping(tokenizer, word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. 训练和测试\n",
    "sample_review = X[57]\n",
    "# print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    return dt_string\n",
    "\n",
    "def epoch_performance(history, model_name):\n",
    "    print(\"model name : {}\".format(model_name))\n",
    "    for i in history.epoch:\n",
    "        print(\"epoch {}\".format(i))\n",
    "        print(\"train acc : {} validate acc : {}\".format(history.history[\"acc\"][i], history.history[\"val_acc\"][i]))\n",
    "        print(\"train loss : {} validate loss : {}\".format(history.history[\"loss\"][i], history.history[\"val_loss\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model = Sequential()\n",
    "    \"\"\"\n",
    "    All that the Embedding layer does is to map the integer inputs to the vectors found at the\n",
    "    corresponding index in the embedding matrix,\n",
    "    i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "    这种模型overfitting比较严重。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    # 光通过这个解决不了overfitting的问题\n",
    "    # model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file , show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    epoch_performance(history, model_name)\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training simple model\")\n",
    "model1 = simple_model(\"simple_model\" ,vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_many_to_one_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model\n",
    "\n",
    "def LSTM_many_to_one_model_v2(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    降低模型的overfit。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    # 减少LSTM的个数\n",
    "    model.add(LSTM(32))\n",
    "    # 加入dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training LSTM v1 model\")\n",
    "model3 = LSTM_many_to_one_model(\"LSTM_v1\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "print(\"start training LSTM v2 model\")\n",
    "model4 = LSTM_many_to_one_model_v2(\"LSTM_v2\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BI_LSTM_many_to_one_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    降低模型的overfit。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training BI_LSTM model\")\n",
    "model5 = BI_LSTM_many_to_one_model(\"BI_LSTM_v1\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def LSTM_many_to_one_model_v3(model_name, dict_size, embedding_matrix, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    变长input，但是实际上通过不了。至少在tensorflow 1.15版本是不行的。\n",
    "    无论是Sequential()还是function API。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    \n",
    "    embedding_input = Input(shape=(None,))\n",
    "    embedding_output = Embedding(dict_size, 100, weights=[embedding_matrix], trainable=False, input_length=None)(embedding_input)\n",
    "    LSTM_output = LSTM(32)(embedding_output)\n",
    "    dropout_output = Dropout(0.5)(LSTM_output)\n",
    "    dense_output = Dense(1, activation='sigmoid')(dropout_output)\n",
    "    \n",
    "    model = Model(embedding_input, dense_output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "        \n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际上无法运行\n",
    "# print(\"start training LSTM v3 model\")\n",
    "# model6 = LSTM_many_to_one_model_v3(\"LSTM_v3\", vocab_size, embedding_mapping, X_train, X_test, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
