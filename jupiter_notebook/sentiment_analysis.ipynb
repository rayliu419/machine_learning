{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是用预训练的word vector来构造DN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "from numpy import array\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras import backend as k\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM\n",
    "\n",
    "\n",
    "#stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_en = set()\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def column_text_to_sentence_array(df, column):\n",
    "    sentence_array = []\n",
    "    word_num = 0\n",
    "    uniq_word_num = 0\n",
    "    word_set = set()\n",
    "    for line in df[column]:\n",
    "        temp = []\n",
    "        for word in word_tokenize(line):\n",
    "                if word not in stopwords_en:\n",
    "                    word_num += 1\n",
    "                    word_set.add(word)\n",
    "                    temp.append(word.lower())\n",
    "        sentence_array.append(temp)\n",
    "    uniq_word_num = len(word_set)\n",
    "    print(\"total word : {}\".format(word_num))\n",
    "    print(\"uniq word : {}\".format(uniq_word_num))\n",
    "    return sentence_array\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(\"./input_data/IMDB_Dataset.csv\")\n",
    "movie_reviews.info()\n",
    "# sns.countplot(x='sentiment', data=movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步先将text文本清理，转换sentiment 0/1\n",
    "movie_reviews[\"review\"] = movie_reviews[\"review\"].map(lambda x: preprocess_text(x))\n",
    "movie_reviews[\"sentiment\"] = movie_reviews[\"sentiment\"].map(lambda x: 1 if x==\"positive\" else 0)\n",
    "\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long time\n",
    "# 第二步使用gensim训练词向量\n",
    "data = column_text_to_sentence_array(movie_reviews, \"review\")\n",
    "model_COBW = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "print(\"finish\")\n",
    "\n",
    "word_vector = model_COBW.wv\n",
    "# like word -> vector dict\n",
    "# word_vector[\"man\"]\n",
    "print(len(word_vector.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = movie_reviews[\"review\"]\n",
    "Y = movie_reviews[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_train[1])\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三步将word数组转换为int数组\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# 上面只保存5000个频率最高的映射？\n",
    "print(len(tokenizer.word_index))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_train[1])\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sequence model前要填充成一样的长度\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立int -> 词向量的映射\n",
    "# [\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] -> [[0.1, 0.4], [0.21, 0.233] ...]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "# tokenizer.word_index 存着[\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] 的映射\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vector[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    except KeyError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# All that the Embedding layer does is to map the integer inputs to the vectors found at the \n",
    "# corresponding index in the embedding matrix, \n",
    "# i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Y_train))\n",
    "print(type(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if isinstance(Y_train, pd.core.series.Series):\n",
    "    Y_train = Y_train.to_numpy()\n",
    "if isinstance(Y_test, pd.core.series.Series):\n",
    "    Y_test = Y_test.to_numpy()\n",
    "# 这里应该是自动完成了映射X_train[0] = [1, 3, 4, 5, 7] ->[word_vector[1],word_vector[3]...]\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_split=0.2)\n",
    "\n",
    "def epoch_performance(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "epoch_performance(history)\n",
    "    \n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score:\", score)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model2 = Sequential()\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(128))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model2.summary())\n",
    "\n",
    "history2 = model2.fit(X_train, Y_train, batch_size=128, epochs=3, verbose=1, validation_split=0.2)\n",
    "epoch_performance(history2)\n",
    "    \n",
    "score2, accuracy2 = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test Score:\", score2)\n",
    "print(\"Test Accuracy:\", accuracy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(instance, model):\n",
    "    instance = tokenizer.texts_to_sequences(instance)\n",
    "    flat_list = []\n",
    "    for sublist in instance:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    flat_list = [flat_list]\n",
    "    instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "    print(model.predict(instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = X[57]\n",
    "predict_single(instance, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
