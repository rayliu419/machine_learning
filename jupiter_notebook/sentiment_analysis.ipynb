{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是用预训练的word vector来构造DN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def column_text_to_sentence_array(df, column):\n",
    "    sentence_array = []\n",
    "    word_num = 0\n",
    "    uniq_word_num = 0\n",
    "    word_set = set()\n",
    "    for line in df[column]:\n",
    "        temp = []\n",
    "        for word in word_tokenize(line):\n",
    "                if word not in stopwords_en:\n",
    "                    word_num += 1\n",
    "                    word_set.add(word)\n",
    "                    temp.append(word.lower())\n",
    "        sentence_array.append(temp)\n",
    "    uniq_word_num = len(word_set)\n",
    "    print(\"total word : {}\".format(word_num))\n",
    "    print(\"uniq word : {}\".format(uniq_word_num))\n",
    "    return sentence_array\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(\"./input_data/IMDB_Dataset.csv\")\n",
    "movie_reviews.info()\n",
    "# sns.countplot(x='sentiment', data=movie_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步先将text文本清理，转换sentiment 0/1\n",
    "movie_reviews[\"review\"] = movie_reviews[\"review\"].map(lambda x: preprocess_text(x))\n",
    "movie_reviews[\"sentiment\"] = movie_reviews[\"sentiment\"].map(lambda x: 1 if x==\"positive\" else 0)\n",
    "\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long time\n",
    "# 第二步使用gensim训练词向量\n",
    "data = column_text_to_sentence_array(movie_reviews, \"review\")\n",
    "model_COBW = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5) \n",
    "print(\"finish\")\n",
    "\n",
    "word_vector = model_COBW.wv\n",
    "# like word -> vector dict\n",
    "word_vector[\"man\"]\n",
    "print(len(word_vector.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = movie_reviews[\"review\"]\n",
    "Y = movie_reviews[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_train[1])\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三步将word数组转换为int数组\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# 上面只保存5000个频率最高的映射？\n",
    "print(len(tokenizer.word_index))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_train[1])\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用sequence model前要填充成一样的长度\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立int -> 词向量的映射\n",
    "# [\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] -> [[0.1, 0.4], [0.21, 0.233] ...]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "# tokenizer.word_index 存着[\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] 的映射\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = word_vector[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# All that the Embedding layer does is to map the integer inputs to the vectors found at the \n",
    "# corresponding index in the embedding matrix, \n",
    "# i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 这里应该是自动完成了映射X_train[0] = [1, 3, 4, 5, 7] ->[word_vector[1],word_vector[3]...]\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
