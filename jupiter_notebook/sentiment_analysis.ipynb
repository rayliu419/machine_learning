{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是用预训练的word vector来构造DN\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import zeros\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "stopwords_en = set()\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "max_row = 100\n",
    "\n",
    "\n",
    "def column_text_to_sentence_array(lines):\n",
    "    sentence_array = []\n",
    "    word_num = 0\n",
    "    word_set = set()\n",
    "    for line in lines:\n",
    "        temp = []\n",
    "        for word in word_tokenize(line):\n",
    "            if word not in stopwords_en:\n",
    "                word_num += 1\n",
    "                word_set.add(word)\n",
    "                temp.append(word.lower())\n",
    "        sentence_array.append(temp)\n",
    "    uniq_word_num = len(word_set)\n",
    "    print(\"total word : {}\".format(word_num))\n",
    "    print(\"unique word : {}\".format(uniq_word_num))\n",
    "    return sentence_array\n",
    "\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def epoch_performance(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def simple_model(dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model = Sequential()\n",
    "    \"\"\"\n",
    "    All that the Embedding layer does is to map the integer inputs to the vectors found at the\n",
    "    corresponding index in the embedding matrix,\n",
    "    i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "    \"\"\"\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file='simple_model.png', show_shapes=True)\n",
    "    # if isinstance(Y, pd.core.series.Series):\n",
    "    #     Y_train = Y.to_numpy()\n",
    "    # if isinstance(Y_test, pd.core.series.Series):\n",
    "    #     Y_test = Y_test.to_numpy()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=10, verbose=1, validation_split=0.2)\n",
    "    epoch_performance(history)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model\n",
    "\n",
    "\n",
    "def LSTM_many_to_one_model(dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model2 = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model2.add(embedding_layer)\n",
    "    model2.add(LSTM(128))\n",
    "    model2.add(Dense(1, activation='sigmoid'))\n",
    "    model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model2, to_file='LSTM_many_to_one_model.png', show_shapes=True)\n",
    "    history2 = model2.fit(X_train, Y_train, batch_size=128, epochs=3, verbose=1, validation_split=0.2)\n",
    "    epoch_performance(history2)\n",
    "    score2, accuracy2 = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score2)\n",
    "    print(\"Test Accuracy:\", accuracy2)\n",
    "    return model2\n",
    "\n",
    "\n",
    "def predict_single(review, model):\n",
    "    review = tokenizer.texts_to_sequences(review)\n",
    "    flat_list = []\n",
    "    for sublist in review:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    flat_list = [flat_list]\n",
    "    review = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "    print(model.predict(review))\n",
    "\n",
    "\n",
    "def build_int_to_vector_mapping(tokenizer, word_vector):\n",
    "    \"\"\"\n",
    "    现在有两个映射\n",
    "    1. gensim 的 word->vector 存在word_vector\n",
    "    2. tokenizer 的 word->int 存在tokenizer\n",
    "    矩阵中存放 int->vector的映射\n",
    "    最终的映射是 [\"I\" \"am\" \"a\" \"super\" \"star\"] -> [1, 3, 4, 5, 7] -> [[0.1, 0.4], [0.21, 0.233] ...]\n",
    "    在Embedding layer embedding matrix被传入\n",
    "    \"\"\"\n",
    "    embedding_matrix = zeros((vocab_size, 100))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vector[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "        except KeyError:\n",
    "            print(\"word {} is OOV\".format(word))\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# 1. 读取数据\n",
    "movie_reviews = pd.read_csv(\"./input_data/IMDB_Dataset.csv\")\n",
    "movie_reviews.info()\n",
    "movie_reviews = movie_reviews[0:max_row]\n",
    "# 2. 预处理数据\n",
    "movie_reviews[\"review\"] = movie_reviews[\"review\"].map(lambda x: preprocess_text(x))\n",
    "movie_reviews[\"sentiment\"] = movie_reviews[\"sentiment\"].map(lambda x: 1 if x==\"positive\" else 0)\n",
    "movie_reviews.head()\n",
    "X = movie_reviews[\"review\"]\n",
    "Y = movie_reviews[\"sentiment\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "\n",
    "# 3. 使用gensim训练词向量\n",
    "data = column_text_to_sentence_array(movie_reviews[\"review\"])\n",
    "model_COBW = gensim.models.Word2Vec(data, min_count=1, size=100, window=5)\n",
    "word_vector = model_COBW.wv\n",
    "# like word -> vector dict\n",
    "print(\"total unique words : {}\".format(len(word_vector.index2word)))\n",
    "\n",
    "# 4. 将word数组转换为int数组，只保存5000个频率最高的映射？\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\"\"\"\n",
    "tokenizer.fit_on_texts(X_train) 这里不能这么写，要用data来fit_on_texts。\n",
    "原因是nltk.tokenize的word_tokenize和keras.preprocessing.fit_on_texts对于有些词不一样，例如cannot, nltk处理成了can和not，但是\n",
    "fit_on_text的时候将 cannot当成一个词了\n",
    "\"\"\"\n",
    "\n",
    "print(len(tokenizer.word_index))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 5. 填充成一样的长度\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train[1]))\n",
    "\n",
    "# 6. 获取输入层的weights\n",
    "embedding_mapping = build_int_to_vector_mapping(tokenizer, word_vector)\n",
    "\n",
    "# 6. 训练和测试\n",
    "sample_review = X[57]\n",
    "print(sample_review)\n",
    "# simple model\n",
    "model1 = simple_model(vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)\n",
    "predict_single(sample_review, model1)\n",
    "\n",
    "model2 = simple_model(vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)\n",
    "predict_single(sample_review, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i am god': 1, 'you are idiot': 2}\n",
      "{'i': 1, 'am': 2, 'god': 3, 'you': 4, 'are': 5, 'idiot': 6}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def correct_and_wrong():\n",
    "    # fit_on_texts错误的用法\n",
    "    tokenizer2 = Tokenizer(num_words=5000)\n",
    "    tokenizer2.fit_on_texts([[\"I am god\"], [\"you are idiot\"]])\n",
    "    print(tokenizer2.word_index)\n",
    "    # fit_on_texts正确的用法\n",
    "    tokenizer3 = Tokenizer(num_words=5000)\n",
    "    tokenizer3.fit_on_texts([[\"I\", \"am\", \"god\"], [\"you\", \"are\", \"idiot\"]])\n",
    "    print(tokenizer3.word_index)\n",
    "    # gensim.models.Word2Vec的错误用法\n",
    "    word_model2 = gensim.models.Word2Vec([[\"I am god\"], [\"you are idiot\"]], min_count=1, size=5, window=5)\n",
    "    word_model2.wv.save_word2vec_format(\"wrong_embedding\")\n",
    "    # gensim.models.Word2Vec的正确用法\n",
    "    word_model3 = gensim.models.Word2Vec([[\"I\", \"am\", \"god\"], [\"you\", \"are\", \"idiot\"]], min_count=1, size=5, window=5)\n",
    "    word_model3.wv.save_word2vec_format(\"correct_embedding\")\n",
    "    \n",
    "correct_and_wrong()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
