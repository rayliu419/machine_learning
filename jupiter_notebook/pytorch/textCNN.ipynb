{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "textCNN做电影的情感分类任务\n",
    "提升精度的方法\n",
    "    1. 增加embedding size，明显\n",
    "    2. 增加sample，明显\n",
    "    3. 增加filter number，不明显。\n",
    "    4. 增加epoch，明显。\n",
    "TextCNN在同样的参数下效果不如LSTM的效果，仅仅能达到80%左右。\n",
    "主要是通过textCNN对CNN有更好的理解，包括初始化，input，output等。\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../common/\")\n",
    "import english_preprocess\n",
    "import movie_review_helper\n",
    "import probability_utils\n",
    "import model_utils\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(TextCNN, self).__init__()\n",
    "        print(args)\n",
    "        filter_num = args[\"filter_num\"]\n",
    "        filter_sizes = args[\"filter_sizes\"]\n",
    "        embedding_size = args[\"embedding_size\"]\n",
    "        drop_out_ratio = args[\"dropout\"]\n",
    "        class_num = args[\"class_num\"]\n",
    "        \"\"\"\n",
    "        Conv2d初始化参数\n",
    "            in_channels - NLP中是几种embedding方式\n",
    "            out_channels - filter的数目，多个filter可能是捕捉多种不同长度的模式\n",
    "            kernel_size - NLP中是ngram的(n, embedding_size)\n",
    "            假设是in_channles = 1，NLP中代表使用gensim带来的embedding\n",
    "            out_channels = 2个filter，表示捕捉两种模式\n",
    "            kernel_size([2, 3], 2)\n",
    "            代表的是计算2gram和3gram的关联，每个word embedding size=2\n",
    "            注意Conv2d的sample size不属于初始化的参数。\n",
    "        \"\"\"\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, filter_num, (cur, embedding_size)) for cur in filter_sizes])\n",
    "        self.dropout = nn.Dropout(drop_out_ratio)\n",
    "        self.fc = nn.Linear(filter_num * len(filter_sizes), class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x是(sample_num, sentence_len, embedding_size)\n",
    "        out = x\n",
    "        out = out.unsqueeze(1)\n",
    "        # out是(sample_num, 1, sentence_len, embedding_size)为了满足Conv2d的输入\n",
    "        out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        \"\"\"\n",
    "        ngram的n=某个值的卷积和max_pool，一次计算多个sample，多个filter\n",
    "        Conv2d的输入解释：\n",
    "            N - batch的大小\n",
    "            C - 通道数量, 这里应该与Conv2d的in_channels是一样的。out = out.unsqueeze(1)就是为了\n",
    "            改变维度来适应这个。\n",
    "            H - 输入的高度，即句子长度\n",
    "            W - 输入的宽度，即embedding的大小\n",
    "        Conv2d的输出\n",
    "            N - batch大小，跟输入的N是一样的。\n",
    "            C - 根据例子来看，是跟Conv2d初始化的filter_num是一样的，与Conv2d的out_channels一样。\n",
    "            H - 高度，在textCNN中其实就是filter在单个sample输入滑动产生的输出。\n",
    "            W - 在textCNN中，应该就是1\n",
    "        :param x:\n",
    "        :param conv:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # x - (8, 1, 5, 2), 8 - sample, 1 - 编码方式个数， 5 - 句子长度，2 - embedding空间大小\n",
    "        output = conv(x)\n",
    "        # output - (8, 2, 4, 1) 8 - sample, 2 - 每个filter的结果， 4 - 每个filter卷积的结果， 1 - 卷积结果的维度\n",
    "        output = F.relu(output)\n",
    "        # 这里的两个squeeze其实都是为了最后产生一个(sample, filter_num)的shape\n",
    "        output = output.squeeze(3)\n",
    "        # output - (8, 2, 4)\n",
    "        output = F.max_pool1d(output, output.size(2))\n",
    "        # output - (8, 2, 1)，每个filter产生一个4 * 1的结果，在4行里取最大值，代表的是模式抓取的语义\n",
    "        output = output.squeeze(2)\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_embedding_samples(samples, embedding_mapping, sentence_length, embedding_size):\n",
    "    samples_mapping = torch.empty((len(samples), sentence_length, embedding_size))\n",
    "    sample_index = 0\n",
    "    for sample in samples:\n",
    "        cur_sample = []\n",
    "        for index in sample:\n",
    "            index_embedding = embedding_mapping[index]\n",
    "            cur_sample.append(index_embedding)\n",
    "        sample_tensor = torch.from_numpy(np.array(cur_sample))\n",
    "        samples_mapping[sample_index] = sample_tensor\n",
    "        sample_index += 1\n",
    "    return samples_mapping\n",
    "\n",
    "\n",
    "def trainset_compare(before, after, actual):\n",
    "    before_predict = before.argmax(dim=1)\n",
    "    after_predict = after.argmax(dim=1)\n",
    "    before_correct = probability_utils.sum_equal(before_predict, actual)\n",
    "    after_correct = probability_utils.sum_equal(after_predict, actual)\n",
    "    # print(\"batch num - {}, before_prdict - {}, after_predict - {}\".format(len(actual), before_correct, after_correct))\n",
    "\n",
    "\"\"\"\n",
    "用来细致观察CNN的初始化，input，output\n",
    "\"\"\"\n",
    "line_num = 10000\n",
    "sentence_maxlen = 100\n",
    "embedding_len = 100\n",
    "filter_num = 8\n",
    "dropout = 0.3\n",
    "epochs = 100\n",
    "lr = 0.002\n",
    "ngram_size = [2, 4, 6]\n",
    "args = {\n",
    "    \"line_num\": line_num,\n",
    "    \"sentence_maxlen\": sentence_maxlen,\n",
    "    \"class_num\": 2,\n",
    "    \"filter_sizes\": ngram_size,\n",
    "    \"embedding_size\": embedding_len,\n",
    "    \"filter_num\": filter_num,\n",
    "    \"dropout\": dropout,\n",
    "    \"lr\": lr,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "print(\"#0 load movie review data\")\n",
    "tokenizer, embedding_mapping, _, _, X_train_np, X_test_np, Y_train_np, Y_test_np = \\\n",
    "    movie_review_helper.prepare_movie_review_for_task(line_num, sentence_maxlen, embedding_len)\n",
    "\n",
    "X_train_embedding_tensor = get_embedding_samples(X_train_np, embedding_mapping, sentence_maxlen, embedding_len)\n",
    "X_test_embedding_tensor = get_embedding_samples(X_test_np, embedding_mapping, sentence_maxlen, embedding_len)\n",
    "Y_train_tensor = torch.from_numpy(Y_train_np).type(torch.LongTensor)\n",
    "Y_test_tensor = torch.from_numpy(Y_test_np).type(torch.LongTensor)\n",
    "\n",
    "print(\"#1 create textCNN\")\n",
    "textCNN = TextCNN(args)\n",
    "model_utils.model_parameters_number(textCNN)\n",
    "\n",
    "print(\"# 2 create optimizer and loss\")\n",
    "optimizer = torch.optim.SGD(textCNN.parameters(), lr=lr)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"#3 batch loader creating\")\n",
    "train_data = data_utils.TensorDataset(X_train_embedding_tensor, Y_train_tensor)\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"#4 start to train\")\n",
    "print(args)\n",
    "for i in range(epochs):\n",
    "    for index, data in enumerate(train_loader):\n",
    "        X_batch_tensor, Y_batch_tensor = data\n",
    "        net_output_tensor = textCNN(X_batch_tensor)\n",
    "        loss = loss_func(net_output_tensor, Y_batch_tensor)\n",
    "        before = net_output_tensor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        after = textCNN(X_batch_tensor)\n",
    "        trainset_compare(before, after, Y_batch_tensor)\n",
    "    if i % 10 == 0:\n",
    "        Y_test_predict_net_output = textCNN(X_test_embedding_tensor)\n",
    "        Y_test_predict = Y_test_predict_net_output.argmax(dim=1)\n",
    "        total = len(Y_test_tensor)\n",
    "        correct_prediction = probability_utils.sum_equal(Y_test_tensor, Y_test_predict)\n",
    "        accuracy = float(correct_prediction) / total\n",
    "        print(\"total - {}, epoch - {}, accuracy - {}\".format(total, i, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
