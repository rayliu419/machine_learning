{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "比较numpy和torch tensor操作\n",
    "其他的math运算见http://pytorch.org/docs/torch.html#math-operations\n",
    "也可以https://pytorch.org/docs/stable/torch.html来查找定义\n",
    "需要熟悉其中的一些操作，这样才能自己实现一些东西\n",
    "一些基本操作都要放在这个文件\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dot \n",
      "numpy: \n",
      " [[ 7 10]\n",
      " [15 22]] \n",
      "torch: \n",
      " tensor(30.)\n",
      "\n",
      "dim 0 cat\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.],\n",
      "        [1., 2.],\n",
      "        [4., 5.],\n",
      "        [1., 2.],\n",
      "        [4., 5.]])\n",
      "\n",
      "dim 1 cat\n",
      "tensor([[1., 2., 1., 2., 1., 2.],\n",
      "        [4., 5., 4., 5., 4., 5.]])\n",
      "\n",
      "origin tensor\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.],\n",
      "        [7., 8.]])\n",
      "\n",
      "dim 0 reduce\n",
      "tensor([12., 15.])\n",
      "\n",
      "dim 1 reduce\n",
      "tensor([ 3.,  9., 15.])\n",
      "tensor([[0.1000, 0.2000, 0.7000],\n",
      "        [0.4000, 0.5000, 0.1000]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.7000, 0.2000],\n",
      "        [0.5000, 0.4000]]),\n",
      "indices=tensor([[2, 1],\n",
      "        [1, 0]]))\n"
     ]
    }
   ],
   "source": [
    "# dot，注意numpy中的dot操作其实跟矩阵乘法类似，但是torch的tensor操作是不一样的。\n",
    "data = [[1,2], [3,4]]\n",
    "tensor = torch.FloatTensor(data)  # 32-bit floating point\n",
    "data = np.array(data)\n",
    "tensor_flatten = torch.flatten(tensor)\n",
    "print(\n",
    "    '\\ndot',\n",
    "    '\\nnumpy: \\n', data.dot(data),     # [[7, 10], [15, 22]]\n",
    "    #'\\ntorch: \\n', torch.dot(tensor, tensor)   # 现在二维的已经不支持直接这样做了。\n",
    "    '\\ntorch: \\n', torch.dot(tensor_flatten, tensor_flatten)  # 30\n",
    ")\n",
    "\n",
    "# cat\n",
    "\"\"\"\n",
    "对于2d，可以简单的认为\n",
    "dim0的cat是将添加行+重新组织矩阵，dim1的cat是添加列+重新组织矩阵\n",
    "\"\"\"\n",
    "tensor_2d = torch.Tensor([[1,2],[4,5]]) \n",
    "dim0_cat = torch.cat((tensor_2d, tensor_2d, tensor_2d), 0)\n",
    "print(\"\\ndim 0 cat\")\n",
    "print(dim0_cat)\n",
    "dim1_cat = torch.cat((tensor_2d, tensor_2d, tensor_2d), 1)\n",
    "print(\"\\ndim 1 cat\")\n",
    "print(dim1_cat)\n",
    "\n",
    "# reduce操作\n",
    "tensor_2d = torch.Tensor([[1,2],[4,5],[7,8]]) \n",
    "print(\"\\norigin tensor\")\n",
    "print(tensor_2d)\n",
    "print(\"\\ndim 0 reduce\")\n",
    "print(torch.sum(tensor_2d, dim=0))\n",
    "print(\"\\ndim 1 reduce\")\n",
    "print(torch.sum(tensor_2d, dim=1))\n",
    "\n",
    "# topk，这种输入一般是比较常见的，就是传出来多个预测概率数组，一次计算所有的。\n",
    "tensorx = torch.Tensor([[.1, .2, .7], [.4, .5, .1]])\n",
    "print(tensorx)\n",
    "# 这里跟我想的有点不一样，dim=1返回的才是想要的。\n",
    "print(torch.topk(tensorx, 2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1d index and slicing\n",
      "tensor([3., 4.])\n",
      "tensor([3., 4.])\n",
      "tensor([[3., 4.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "\n",
      "2d index and slicing\n",
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2., 3.]])\n",
      "tensor([[4., 5., 6.]])\n",
      "tensor([[7., 8., 9.]])\n",
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [7.]])\n",
      "tensor([[2.],\n",
      "        [5.],\n",
      "        [8.]])\n",
      "tensor([[3.],\n",
      "        [6.],\n",
      "        [9.]])\n",
      "tensor([[5., 6.],\n",
      "        [8., 9.]])\n",
      "\n",
      "abs \n",
      "numpy: \n",
      " [1 2 1 2] \n",
      "torch: \n",
      " tensor([1., 2., 1., 2.])\n",
      "\n",
      "2d abs \n",
      "numpy: \n",
      " [[1 2]\n",
      " [1 2]] \n",
      "torch: \n",
      " tensor([[1., 2.],\n",
      "        [1., 2.]])\n",
      "\n",
      "mean \n",
      "numpy: \n",
      " 0.0 \n",
      "torch: \n",
      " tensor(0.)\n",
      "\n",
      "2d mean \n",
      "numpy: \n",
      " 0.0 \n",
      "torch: \n",
      " tensor(0.)\n",
      "\n",
      "add\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 10.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 10.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 10.]])\n",
      "\n",
      "tensor *\n",
      "tensor([[ 1.,  4.],\n",
      "        [16., 25.]])\n",
      "\n",
      "element equal\n",
      "tensor([[ True,  True],\n",
      "        [ True,  True],\n",
      "        [ True, False]])\n",
      "\n",
      "# Matrix X vector\n",
      "tensor([2., 5.])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1d index and slicing\")\n",
    "# 1d index and slicing\n",
    "print(tensor[1])\n",
    "print(tensor[-1])\n",
    "print(tensor[1:])\n",
    "# 2d index index and slicing\n",
    "tensor_2d = torch.Tensor([[1,2,3],[4,5,6], [7,8,9]]) \n",
    "print(tensor_2d)\n",
    "print(\"\\n2d index and slicing\")\n",
    "print(tensor_2d[0])\n",
    "# 左闭合右开，如果两边相等，则不取\n",
    "print(tensor_2d[0:1, :])\n",
    "print(tensor_2d[1:2, :])\n",
    "print(tensor_2d[2:3, :])\n",
    "print(tensor_2d[:, 0:1])\n",
    "print(tensor_2d[:, 1:2])\n",
    "print(tensor_2d[:, 2:3])\n",
    "# 最右下角\n",
    "print(tensor_2d[1:3, 1:3])\n",
    "\n",
    "# abs\n",
    "data = [-1, -2, 1, 2]\n",
    "tensor = torch.FloatTensor(data)  # 32-bit floating point\n",
    "print(\n",
    "    '\\nabs',\n",
    "    '\\nnumpy: \\n', np.abs(data),          # [1 2 1 2]\n",
    "    '\\ntorch: \\n', torch.abs(tensor)      # [1 2 1 2]\n",
    ")\n",
    "data_2d = np.array([-1, -2, 1, 2]).reshape((2,2))\n",
    "tensor_2d = torch.FloatTensor(data_2d)\n",
    "print(\n",
    "    '\\n2d abs',\n",
    "    '\\nnumpy: \\n', np.abs(data_2d),         \n",
    "    '\\ntorch: \\n', torch.abs(tensor_2d)     \n",
    ")\n",
    "\n",
    "# mean\n",
    "data = [-1, -2, 1, 2]\n",
    "tensor = torch.FloatTensor(data)  # 32-bit floating point\n",
    "print(\n",
    "    '\\nmean',\n",
    "    '\\nnumpy: \\n', np.mean(data),         # 0.0\n",
    "    '\\ntorch: \\n', torch.mean(tensor)     # 0.0\n",
    ")\n",
    "\n",
    "data_2d = np.array([-1, -2, 1, 2]).reshape((2,2))\n",
    "tensor_2d = torch.FloatTensor(data_2d)\n",
    "print(\n",
    "    '\\n2d mean',\n",
    "    '\\nnumpy: \\n', np.mean(data_2d),         # 0.0\n",
    "    '\\ntorch: \\n', torch.mean(tensor_2d)     # 0.0\n",
    ")\n",
    "\n",
    "# add, 基于元素的\n",
    "print(\"\\nadd\")\n",
    "x1 = torch.Tensor([[1,2],[4,5]])  \n",
    "x2 = torch.Tensor([[1,2],[4,5]]) \n",
    "y1 = x1 + x2\n",
    "y2 = torch.add(x1, x2)\n",
    "y3 = x1.add(x2)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "\n",
    "# *，基于元素的\n",
    "print(\"\\ntensor *\")\n",
    "tensor_2d = torch.Tensor([[1,2],[4,5]]) \n",
    "print(tensor_2d * tensor_2d)\n",
    "\n",
    "# 元素级别的比较\n",
    "tensor_2d_1 = torch.Tensor([[1,2],[4,5],[7,8]]) \n",
    "tensor_2d_2 = torch.Tensor([[1,2],[4,5],[7,9]]) \n",
    "print(\"\\nelement equal\")\n",
    "print(torch.eq(tensor_2d_1,tensor_2d_2))\n",
    "\n",
    "\n",
    "# 这个操作比较违反直觉\n",
    "print(\"\\n# Matrix X vector\")\n",
    "tensor1 = torch.Tensor([[1,2,3], [4,5,6]]) # 2*3\n",
    "tensor2 = torch.Tensor([0,1,0]) # 1*3\n",
    "print(torch.mv(tensor1, tensor2)) # 1*2, mv - matrix vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# is_tensor\n",
    "print(torch.is_tensor(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 3]])\n",
    "pad = 0\n",
    "result = (x != pad)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix x Matrix\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "torch.mm\n",
      "tensor([[2.],\n",
      "        [5.]])\n",
      "torch.matmut\n",
      "tensor([[2.],\n",
      "        [5.]])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([1, 3, 1])\n",
      "tensor([[[2.],\n",
      "         [5.]]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "\"\"\"\n",
    "矩阵乘法在什么地方用到？\n",
    "1. 批量sample attention的query与key计算。\n",
    "2. 批量计算attention的attention score与value的。\n",
    "\"\"\"\n",
    "print(\"\\nMatrix x Matrix\")\n",
    "tensor1 = torch.Tensor([[1,2,3], [4,5,6]])  # 2*3的矩阵\n",
    "tensor2 = torch.Tensor([[0],[1],[0]]) # 3*1矩阵\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "print(\"torch.mm\")\n",
    "# 2*1的矩阵, mm - 标准矩阵乘法\n",
    "print(torch.mm(tensor1, tensor2)) \n",
    "print(\"torch.matmut\")\n",
    "print(torch.matmul(tensor1, tensor2))\n",
    "# 增加维度\n",
    "tensor1_reshape = torch.unsqueeze(tensor1, 0)\n",
    "tensor2_reshape = torch.unsqueeze(tensor2, 0)\n",
    "print(tensor1_reshape.shape)\n",
    "print(tensor2_reshape.shape)\n",
    "# error! mm只能完成二维矩阵的乘法\n",
    "# print(torch.mm(tensor1_reshape, tensor2_reshape))\n",
    "# matmul可以完成高维向量相乘，只要后面两维满足矩阵乘法规则。例如(1, 2, 3), (1, 3, 1) = (1, 2, 1)\n",
    "# (1, 1, 2, 3) 和 (1, 1, 3, 1)也可以\n",
    "print(torch.matmul(tensor1_reshape, tensor2_reshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.Tensor([[1,2,3], [4,5,6]])  # 2*3的矩阵\n",
    "tensor2 = torch.Tensor([[-1, -2, -3]]) # 3*1矩阵\n",
    "print(tensor1 + tensor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
