{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variable in torch is to build a computational graph,\n",
    "# but this graph is dynamic compared with a static graph in Tensorflow or Theano.\n",
    "# So torch does not have placeholder, torch can just pass variable to the computational graph.\n",
    "\"\"\"\n",
    "Variable是对tensor的封装。\n",
    "Variable有三个属性：\n",
    ".data：tensor本身\n",
    ".grad：对应tensor的梯度\n",
    ".grad_fn：该Variable是通过什么方式获得的\n",
    "\"\"\"\n",
    "tensor = torch.FloatTensor([[1,2],[3,4]])            \n",
    "variable = Variable(tensor, requires_grad=True)\n",
    "print(tensor)       \n",
    "print(variable) \n",
    "\n",
    "# variable包含了tensor，如果形成了神经网络，variable可以根据计算图的反向传播求导更新。\n",
    "t_out = torch.mean(tensor*tensor)       # x^2\n",
    "v_out = torch.mean(variable*variable)   # x^2\n",
    "print(t_out)\n",
    "print(v_out)    # 7.5\n",
    "\n",
    "v_out.backward()    # backpropagation from v_out\n",
    "# v_out = 1/4 * sum(variable*variable)\n",
    "# the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2\n",
    "print(variable.grad)\n",
    "\n",
    "print(variable)     # this is data in variable format\n",
    "\n",
    "print(variable.data)\n",
    "\n",
    "# 取出variable里的data转为numpy\n",
    "print(variable.data.numpy())    # numpy format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做假数据\n",
    "x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)\n",
    "x = Variable(x)\n",
    "x_np = x.data.numpy()   # numpy array for plotting\n",
    "\n",
    "# 激活函数可以和接受Variable类型 \n",
    "y_relu = torch.relu(x).data.numpy()\n",
    "y_sigmoid = torch.sigmoid(x).data.numpy()\n",
    "y_tanh = torch.tanh(x).data.numpy()\n",
    "y_softplus = F.softplus(x).data.numpy() # there's no softplus in torch\n",
    "\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.subplot(221)\n",
    "plt.plot(x_np, y_relu, c='red', label='relu')\n",
    "plt.ylim((-1, 5))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(x_np, y_sigmoid, c='red', label='sigmoid')\n",
    "plt.ylim((-0.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(x_np, y_tanh, c='red', label='tanh')\n",
    "plt.ylim((-1.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(x_np, y_softplus, c='red', label='softplus')\n",
    "plt.ylim((-0.2, 6))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(3, 3)\n",
    "x[torch.randn(3, 3) > 0.5] = 1\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.count_nonzero(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
