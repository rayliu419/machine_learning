{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../common/\")\n",
    "import name2countries_helper\n",
    "import data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSample(X_int_encoding, Y_int_encoding):\n",
    "    \"\"\"\n",
    "    抽取单个sample\n",
    "    \"\"\"\n",
    "    index = random.randint(0, len(X_int_encoding) - 1)\n",
    "    return index, X_int_encoding[index], Y_int_encoding[index]\n",
    "\n",
    "def randomSampleTransform(X, Y, X_int_encoding, Y_int_encoding, n_letters, category_num):\n",
    "    \"\"\"\n",
    "    返回抽取的sample，并且转化成tensor格式\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index, cur_x, cur_y = randomSample(X_int_encoding, Y_int_encoding)\n",
    "        cur_x_np = data_preprocessing.one_hot_encoding(cur_x, n_letters)\n",
    "        cur_y_np = data_preprocessing.one_hot_encoding(cur_y, category_num)\n",
    "        cur_x_tensor = torch.from_numpy(cur_x_np)\n",
    "        cur_y_tensor = torch.from_numpy(cur_y_np)\n",
    "    except IndexError:\n",
    "        print(index)\n",
    "    return X[index], Y[index], cur_x_tensor, cur_y_tensor, cur_y\n",
    "\n",
    "def get_category_from_output(output):\n",
    "    top_n_values, top_n_indexes = output.topk(1)\n",
    "    return top_n_values[0].item(), top_n_indexes[0].item()\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    输入当前字符的one-hot表示和上一个hidden state(第一个字符的hidden state是0)\n",
    "    获取的output是各个语言的概率和下一个步骤要用到的hidden state。\n",
    "    这个是一个非常简单的RNN，RNN的核心其实是hidden_state参与到下一个输入，这里也没有用到激活函数\n",
    "    和，对于输入和hidden的处理也没有分别处理，而是直接concat。\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 0)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    \n",
    "def train(x_tensor, y_tensor, rnn, y_int, n_hidden):\n",
    "    \"\"\"\n",
    "    注意：\n",
    "    1. 第一个字符的hidden state总是0。\n",
    "    2. 每次取一个sample，所以这个train是一个个训练的，速度比较慢。\n",
    "    3. 处理的输入是变长的。这个也是为什么要一个个训练。\n",
    "    4. many-to-one的情况，loss最后跟output算。如果是many-to-many，loss什么时候算呢？是有一个output就\n",
    "    算，还是全出来再算?\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden = torch.zeros(n_hidden)\n",
    "    rnn.zero_grad()\n",
    "    for i in range(x_tensor.size(0)):\n",
    "        output, hidden = rnn(x_tensor[i], hidden)\n",
    "    output = output.unsqueeze(dim=0)\n",
    "    target = torch.Tensor([y_int]).long()\n",
    "    loss = loss_func(output, target)\n",
    "    loss.backward()\n",
    "    \"\"\"\n",
    "    Add parameters' gradients to their values, multiplied by learning rate\n",
    "    没有使用optimizer，手动更新参数。\n",
    "    \"\"\"\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    output = output.squeeze(dim=0)\n",
    "    return output, loss.item()\n",
    "\n",
    "\n",
    "def evaluate(rnn, n_hidden, char_tokenizer, n_letters, x_string):\n",
    "    cur_encoding = char_tokenizer.texts_to_sequences([x_string])\n",
    "    cur_x_np = data_preprocessing.one_hot_encoding(cur_encoding, n_letters)\n",
    "    cur_x_tensor = torch.from_numpy(cur_x_np)\n",
    "    cur_x_tensor = cur_x_tensor.squeeze(dim=0)\n",
    "    hidden = torch.zeros(n_hidden)\n",
    "    for i in range(cur_x_tensor.size(0)):\n",
    "        output, hidden = rnn(cur_x_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "\n",
    "def predict_name(rnn, n_hidden, x_string, char_tokenizer, n_letters, int2country, n_predictions=3):\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(rnn, n_hidden, char_tokenizer, n_letters, x_string)\n",
    "        # Get top N categories\n",
    "        softmax_func = nn.Softmax(dim = 0)\n",
    "        soft_max_output = softmax_func(output)\n",
    "        top_values, top_index = soft_max_output.topk(n_predictions)\n",
    "        for i in range(n_predictions):\n",
    "            value = top_values[i].item()\n",
    "            category_index = top_index[i].item()\n",
    "            value = value * 100\n",
    "            value_str = \"{}%\".format(value)\n",
    "            print(\"name - {}, prob - {}, country - {}\".format(x_string, value_str, int2country[category_index]))\n",
    "\n",
    "\n",
    "print(\"#1 load data\")\n",
    "char_tokenizer, n_letters, country_num, X, Y, X_int_encoding, Y_int_encoding, int2country = name2countries_helper.prepare_name2countries_data_for_task()\n",
    "print(n_letters)\n",
    "print(country_num)\n",
    "print(\"#2 init RNN\")\n",
    "n_hidden = 128\n",
    "n_categories = country_num\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "print(\"#3 define parameters\")\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "learning_rate = 5e-4\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 5000\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "\n",
    "print(\"#4 start training\")\n",
    "for iter in range(1, n_iters + 1):\n",
    "    x_ori, y_ori, x_tensor, y_tensor, y_int = randomSampleTransform(X, Y, X_int_encoding, Y_int_encoding, n_letters, n_categories)\n",
    "    output, loss = train(x_tensor, y_tensor, rnn, y_int, n_hidden)\n",
    "    current_loss += loss\n",
    "    predict, predict_index = get_category_from_output(output)\n",
    "    if iter % print_every == 0:\n",
    "        print(iter)\n",
    "        correct = '✓' if predict_index == y_int else '✗ (%s)'\n",
    "        print(\"name - {}, country - {}, actual - {}, {}, predict - {}, {}\".format(x_ori, y_ori, y_int, int2country[y_int], predict_index, int2country[predict_index]))\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0\n",
    "print(\"#5 print loss\")\n",
    "for loss in all_losses:\n",
    "    print(loss)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "\n",
    "print(\"# 5 test single\")\n",
    "predict_name(rnn, n_hidden, 'Dovesky', char_tokenizer, n_letters, int2country)\n",
    "predict_name(rnn, n_hidden, 'Jackson', char_tokenizer, n_letters, int2country)\n",
    "predict_name(rnn, n_hidden, 'Satoshi', char_tokenizer, n_letters, int2country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
