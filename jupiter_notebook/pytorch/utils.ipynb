{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "view()函数经常使用用来调整输入之类的。\n",
    "另外，就想view()暗示的，下游的数据是共享的。改变x，就会改变y和z。\n",
    "\"\"\"\n",
    "x = torch.randn(4, 4)\n",
    "print(x)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "\"\"\"\n",
    "the size -1 表示本维度从其他维度推断\n",
    "\"\"\"\n",
    "z = x.view(-1, 8) \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([4, 3])\n",
    "print(a)\n",
    "b = torch.zeros([4, 3])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor经常用的操作\n",
    "exp - 求exp, 可以输入二维数组\n",
    "sum - 返回标量，不管输入的是几维\n",
    "/ - 元素除\n",
    "\"\"\"\n",
    "c = torch.Tensor([[-1, 1, 2], [2, 2, 4]])\n",
    "c_exp = c.exp()\n",
    "print(c_exp)\n",
    "c_sum = c.sum()\n",
    "print(c_sum)\n",
    "c_divide = c / c_sum\n",
    "print(c_divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tensor常用函数\n",
    "Softmax = exp{xi} / sum(exp{x1~xn})\n",
    "\"\"\"\n",
    "softmax_func = nn.Softmax(dim = 1)\n",
    "c_softmax = softmax_func(c)\n",
    "print(c_softmax)\n",
    "\n",
    "x_input = torch.randn(1, 5)\n",
    "print('x_input:\\n',x_input)\n",
    "y_target = torch.tensor([1])\n",
    "print('y_target\\n',y_target)\n",
    "crossentropyloss = nn.CrossEntropyLoss()\n",
    "crossentropyloss_output = crossentropyloss(x_input, y_target)\n",
    "print(crossentropyloss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "一些权重初始化的方法\n",
    "bias一般用torch.zeors\n",
    "\"\"\"\n",
    "d = torch.randn([10, 10])\n",
    "print(d)\n",
    "\n",
    "d_normal = torch.nn.init.normal(d, 0, 1)\n",
    "print(d_normal)\n",
    "\n",
    "d_uniform = torch.nn.init.uniform(d, 0, 10)\n",
    "print(d_uniform)\n",
    "\n",
    "w = torch.empty(3, 5)\n",
    "nn.init.constant_(w, 0.3)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.Tensor([[-1, 1, 2], [2, 2, 4]])\n",
    "print(f)\n",
    "f = f.float()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "怎么理解squeeze和unsqueeze:\n",
    "If you look at the shape of the array before and after, \n",
    "you see that before it was (4,) and after it is (1, 4) (when second parameter is 0)\n",
    "and (4, 1) (when second parameter is 1). So a 1 was inserted in the shape of the array at axis 0 or 1, \n",
    "depending on the value of the second parameter.\n",
    "That is opposite of np.squeeze() which removes axes of size 1 (singletons).\n",
    "为什么需要squeeze/unsqueeze?我觉得应该是为了满足某些layer的input的shape的要求，老是需要调整\n",
    "shape。\n",
    "\"\"\"\n",
    "g = torch.zeros(4)\n",
    "print(g)\n",
    "# g shape is (4)\n",
    "g_unsqueezz_dim0 = torch.unsqueeze(g, dim=0)\n",
    "# 在(4)的0位置插入一个1，变成(1, 4)\n",
    "g_unsqueezz_dim1 = torch.unsqueeze(g, dim=1)\n",
    "# 在(4)的1位置插入1，变成(4,1)\n",
    "print(g_unsqueezz_dim0)\n",
    "print(g_unsqueezz_dim1)\n",
    "g_squeeze_dim0 = torch.squeeze(g_unsqueezz_dim0, dim=0)\n",
    "print(g_squeeze_dim0)\n",
    "# (1, 4)的0位置移除1，变成(4)\n",
    "g_squeeze_dim1 = torch.squeeze(g_unsqueezz_dim1, dim=1)\n",
    "# (4, 1)位置移除1，变成(4)\n",
    "print(g_squeeze_dim1)\n",
    "\n",
    "h = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(h)\n",
    "h_dim0 = torch.unsqueeze(h, dim=0)\n",
    "print(h_dim0)\n",
    "# (1, 2, 3)\n",
    "h_dim1 = torch.unsqueeze(h, dim=1)\n",
    "print(h_dim1)\n",
    "# (2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss的用法\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(8, 2, requires_grad=True)\n",
    "target = torch.empty(8, dtype=torch.long).random_(2)\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9938)\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor([-2.8961, -2.9042, -2.9938, -2.9543, -2.8207, -2.9718, -2.8264, -2.8576,\n",
    "        -2.8961, -2.7781, -2.8565, -2.9186, -2.9661, -2.7642, -2.9291, -2.8690,\n",
    "        -2.9487, -2.9125])\n",
    "z = z.unsqueeze(0)\n",
    "loss_func = nn.NLLLoss()\n",
    "target = torch.Tensor([2]).long()\n",
    "loss = loss_func(z, target)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
