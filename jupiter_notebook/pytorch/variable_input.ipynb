{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.utils import sequence_utils\n",
    "from common.utils.type_cast_utils import *\n",
    "from common.utils.model_utils import *\n",
    "from common.utils.nlp_utils import *\n",
    "from common.data_loader import movie_review_helper\n",
    "from common.lng_processor.eng_process import *\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"\n",
    "演示怎么正确处理变长类型的输入，不使用截断的方法，而是使用padding + masking。\n",
    "正确理解torch.nn.LSTM的输入和输出。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "新的电影评论在这个上面的性能不好。\n",
    "从优化的速度和效果来看，耗时间长且优化的loss下降慢，为什么? 可能的原因是:\n",
    "1. 整体的sample数目还是有点少。2000个正例，2000个负例。\n",
    "2. 每个sample的单词数比较多，几百上千个。\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "df = load_movie_review()\n",
    "df['text_remove_punctuations'] = \\\n",
    "    df['text'].apply(lambda x: remove_punctuations_array(x))\n",
    "df.info()\n",
    "\n",
    "word_dict = build_word_to_int_dict(df['text_remove_punctuations'])\n",
    "print(word_dict.total_token)\n",
    "print(word_dict.total_uniq_word)\n",
    "\n",
    "print(\"encoding\")\n",
    "df['encode'] = df['text_remove_punctuations'].apply(lambda x: word_dict.encode_tokenized_sentence(x))\n",
    "\n",
    "print(df.loc[0:0, ['text_remove_punctuations']])\n",
    "print(df.loc[0:0, ['encode']])\n",
    "\n",
    "df.drop(columns=['text_remove_punctuations', 'text'], inplace=True)\n",
    "\n",
    "df['label_encode'] = df['label'].apply(lambda x: 0 if x == 'neg' else 1)\n",
    "df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "print(df.loc[0:1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "老数据集，本地下载的IMDB_dataset.csv。\n",
    "有的时候loss居然也不收敛，跟网络的初始化有关系吗?\n",
    "- 看起来是的，所以要尝试不同的初始权重。有的时候初始loss虽然很大，但是可以持续优化。有的时候初始loss就比较小了，但是会陷入到不能优化的地步。\n",
    "怎么系统的避免这种问题。\n",
    "\n",
    "\"\"\"\n",
    "print(\"loading and cleaning data\")\n",
    "df = movie_review_helper.prepare_movie_review_raw(5000)\n",
    "df['text_remove_punctuations'] = \\\n",
    "    df['text'].apply(lambda x: remove_punctuations_array(x))\n",
    "df.info()\n",
    "\n",
    "word_dict = build_word_to_int_dictionary(df['text_remove_punctuations'])\n",
    "print(word_dict.total_token)\n",
    "print(word_dict.total_uniq_word)\n",
    "\n",
    "print(\"encoding\")\n",
    "df['encode'] = df['text_remove_punctuations'].apply(lambda x: word_dict.encode_tokenized_sentence(x))\n",
    "df['label_encode'] = df['label'].apply(lambda x: 0 if x == 'negative' else 1)\n",
    "\n",
    "print(df.loc[0:0, ['text_remove_punctuations']])\n",
    "print(df.loc[0:0, ['encode']])\n",
    "\n",
    "df.drop(columns=['text_remove_punctuations', 'text', 'label'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableNet(nn.Module):\n",
    "    def __init__(self, word_dict_size, embedding_dimision, lstm_hidden_size):\n",
    "        super(VariableNet, self).__init__()\n",
    "        self.word_dict_size = word_dict_size\n",
    "        self.embedding_dimision = embedding_dimision\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        \"\"\"\n",
    "        padding的embedding编码 - [.0, .0, .0...]\n",
    "        而且看起来不会在backward更新。\n",
    "        \"\"\"\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=word_dict_size,\n",
    "                                            embedding_dim=embedding_dimision, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(input_size=embedding_dimision, hidden_size=lstm_hidden_size,\n",
    "                                  batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        # batch_size * 1\n",
    "        self.fc = torch.nn.Linear(lstm_hidden_size, 16)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(16, 1)\n",
    "        # sigmoid\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意每次计算时，要重新初始化h0和c0。\n",
    "        h0 = torch.zeros(x.size(0), self.lstm_hidden_size).view((1, x.size(0), -1))\n",
    "        c0 = torch.zeros(x.size(0), self.lstm_hidden_size).view((1, x.size(0), -1))\n",
    "        # 为了获取变长的每个输入的长度\n",
    "        sample_number = x.size(0)\n",
    "        # 获取变长的sample的长度\n",
    "        sample_lengths = sequence_utils.count_nonzero(x)\n",
    "        input_embedding = self.embedding(x.long())\n",
    "        \"\"\"\n",
    "        转换输入的格式\n",
    "        把上一个输入整平，把每个sample的第i个time step按顺序合并到一起，同时会有一个batch_sizes记录每个time step有多少个。\n",
    "        例如上例说明第一个time step有2个输入，第二个也是2个，第三个有1个等。\n",
    "        采用这样的方式输入到lstm中，可以提高lstm的计算效率。\n",
    "        \"\"\"\n",
    "        packed_seq_batch = torch.nn.utils.rnn. \\\n",
    "            pack_padded_sequence(input_embedding, lengths=sample_lengths, batch_first=True)\n",
    "        output, (hn, cn) = self.lstm(packed_seq_batch.float(), (h0.detach(), c0.detach()))\n",
    "        \"\"\"\n",
    "        输出也得转回来，这里的问题是：我们不是从padded_output来取结果，而是从hn取结果。原因是因为padded_output对于不是那么长的\n",
    "        序列，最后的一个实际上是padding位了。\n",
    "        而hn, cs会自动考虑padding的问题, 输出是最后的实际有效位置的。\n",
    "        使用实验对比了:\n",
    "        padded_output[1:2, sample_lengths[1]:sample_lengths[1] + 1, :]与hn[1:2]的结果是相同的\n",
    "        即手动将padded_output的元素自己通过有效位置取出来，不过用hn直接取要更加方便。\n",
    "        \"\"\"\n",
    "        padded_output, output_lens = \\\n",
    "            torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        fc_input = hn.view((sample_number, -1))\n",
    "        fc_input = self.dropout(fc_input)\n",
    "        fc_output = self.fc(fc_input)\n",
    "        fc2_input = self.relu(fc_output)\n",
    "        fc2_output = self.fc2(fc2_input)\n",
    "        sigmoid_output = self.sigmoid(fc2_output)\n",
    "        return sigmoid_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create model\")\n",
    "embedding_dimision = 10\n",
    "lstm_hidden_size = 64\n",
    "loss_func = torch.nn.BCELoss()\n",
    "variable_net = VariableNet(word_dict.total_token, embedding_dimision, lstm_hidden_size)\n",
    "optimizer = torch.optim.Adam(variable_net.parameters(), lr=0.01, weight_decay=0.0005)\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "torch_model_parameters_number(variable_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"partition\")\n",
    "df = df.sample(frac=1.0).reset_index(drop=True)\n",
    "total_sample, _ = df.shape\n",
    "partition_num = total_sample / batch_size\n",
    "sub_df = np.array_split(df, partition_num)\n",
    "print(len(sub_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, epochs):\n",
    "    total_loss = 0\n",
    "    print(\"epoch - {}\".format(i))\n",
    "    batch_num = 0\n",
    "    for batch_df in sub_df:\n",
    "        batch_df['sentence_len'] = batch_df['encode'].apply(lambda x: len(x))\n",
    "        batch_df.sort_values('sentence_len', ascending=False, inplace=True)\n",
    "        # X is list of variable list.\n",
    "        X = series_to_list(batch_df['encode'])\n",
    "        Y = series_to_list(batch_df['label_encode'])\n",
    "        X_tensor = var_list_to_tensor(X, 0)\n",
    "        Y_tensor = list_to_tensor(Y)\n",
    "        sample_number, maxlen = X_tensor.shape\n",
    "        Y_tensor_reshape = Y_tensor.view((sample_number, -1))\n",
    "        # 清理上一轮的梯度\n",
    "        optimizer.zero_grad()\n",
    "        output = variable_net(X_tensor)\n",
    "        loss = loss_func(output, Y_tensor_reshape.float())\n",
    "        print(\"epoch - {}, batch - {}, loss - {}\".format(i, batch_num, loss))\n",
    "        # for predict, actual in zip(output, Y_tensor_reshape):\n",
    "        #     print(\"predict 1 - {}, actual - {}\".format(predict, actual))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # output2 = variable_net(X_tensor)\n",
    "        # loss2 = loss_func(output2, Y_tensor_reshape.float())\n",
    "        # for predict, actual in zip(output2, Y_tensor_reshape):\n",
    "        #     print(\"predict 2 - {}, actual - {}, after loss - {}\".format(predict, actual, loss2))\n",
    "        # print()\n",
    "        total_loss += loss\n",
    "        batch_num += 1\n",
    "    print(\"epoch - {}, total loss - {}\".format(i, total_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
