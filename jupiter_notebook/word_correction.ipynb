{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从SQuAD 2.0出去文本数据，加入随机噪音生成训练集。\n",
    "# 比较不同的model的纠错效果。\n",
    "# 英文数据\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from numpy.random import choice as random_choice, randint as random_randint, shuffle as random_shuffle, seed as random_seed\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Embedding\n",
    "\n",
    "english_text_file = \"./input_data/abstract_english_text_file\"\n",
    "clean_english_text_file = \"./input_data/clean_english_text_file\"\n",
    "before_add_error_file = \"./input_data/before_add_error\"\n",
    "after_add_error_file = \"./input_data/after_add_error\"\n",
    "change_index_file = \"./input_data/change_index_file\"\n",
    "\n",
    "err_prob = {\n",
    "    \"replace_one_char\": 0.4,\n",
    "    \"add_one_char\": 0.2,\n",
    "    \"delete_one_char\": 0.2,\n",
    "    \"change_neighbor_order\": 0.2\n",
    "}\n",
    "\n",
    "# max_error_rate = 0.2\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .\")\n",
    "max_error_line_length_rate = 0.05\n",
    "\n",
    "# 函数用来parse SQuAD 2.0的数据\n",
    "# 我们只需要抽取文本信息, SQuAD2.0的数据格式比较奇怪\n",
    "def read_squad_examples(input_file, is_training):\n",
    "    with tf.io.gfile.GFile(input_file, \"r\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "    return input_data\n",
    "    \n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 抽取英文文本，中文和其他类别的去掉\n",
    "def abstarct_sentence(json_data):\n",
    "    global english_text_file\n",
    "    fp = open(english_text_file, \"w\")\n",
    "    for article in json_data:\n",
    "        paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph[\"qas\"]\n",
    "            for qa in qas:\n",
    "                question = qa[\"question\"]\n",
    "                if isEnglish(question):\n",
    "                    fp.write(question + \"\\n\")\n",
    "                answer_struct = qa[\"answers\"]\n",
    "                if (len(answer_struct) > 0):\n",
    "                    answer = answer_struct[0][\"text\"]\n",
    "                    if isEnglish(answer):\n",
    "                        fp.write(answer + \"\\n\")\n",
    "    fp.close()\n",
    "    print(\"abstract finish\")\n",
    "\n",
    "def clean_file(file=english_text_file):\n",
    "    global clean_english_text_file\n",
    "    fp = open(file, \"r\")\n",
    "    fp2 = open(clean_english_text_file, \"w\")\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if line != \"\" and line != \"null\" and len(line) > 10:\n",
    "            fp2.write(line + \"\\n\")\n",
    "    fp.close()\n",
    "    fp2.close()\n",
    "\n",
    "def map_prob_to_range_with_keys(key_prob):\n",
    "    prob_list = list(key_prob.values())\n",
    "    prob_sum = sum(map(float,prob_list))\n",
    "    key_range = dict()\n",
    "    if math.isclose(prob_sum, 1) == False:\n",
    "        print(\"prob sum is not 1\")\n",
    "        sys.exit(-1)\n",
    "    else:\n",
    "        threshold = 0\n",
    "        for key, prob in key_prob.items():\n",
    "            key_range[key] = [threshold, threshold + prob]\n",
    "            threshold += prob\n",
    "    return key_range\n",
    "\n",
    "def choose_item_based_on_prob(key_range):\n",
    "    value = random.uniform(0, 1)\n",
    "    last_key = None\n",
    "    for key, prob_range in key_range.items():\n",
    "        last_key = key\n",
    "        if value >= prob_range[0] and value < prob_range[1]:\n",
    "            return key\n",
    "    return last_key\n",
    "\n",
    "# 随机修改正确的句子到错误的句子\n",
    "# 包括添加字符，删除字符，交换临近字符，替换字符\n",
    "def add_error_to_line(line):\n",
    "    max_error_line_length_rate\n",
    "    max_error_num = (int)(max_error_line_length_rate * len(line))\n",
    "    set_error_num = (int)(random.uniform(0, 1) * max_error_num)\n",
    "    cur_error_num = 0\n",
    "    before = line\n",
    "    after = line\n",
    "    key_range = map_prob_to_range_with_keys(err_prob)\n",
    "    while cur_error_num < set_error_num:\n",
    "        err_type = choose_item_based_on_prob(key_range)\n",
    "        cur_error_num += 1\n",
    "        if err_type == \"replace_one_char\":\n",
    "            random_char_position = random_randint(len(after))\n",
    "            after = after[:random_char_position] + random_choice(char_list[:-1]) \\\n",
    "            + after[random_char_position + 1:]\n",
    "        elif err_type == \"add_one_char\":\n",
    "            random_char_position = random_randint(len(after))\n",
    "            after = after[:random_char_position] + random_choice(char_list[:-1]) \\\n",
    "            + after[random_char_position:]\n",
    "        elif err_type == \"delete_one_char\":\n",
    "            random_char_position = random_randint(len(after))\n",
    "            after = after[:random_char_position] + after[random_char_position + 1:]\n",
    "        elif err_type == \"change_neighbor_order\":\n",
    "            random_char_position = random_randint(len(after) - 1)\n",
    "            after = (after[:random_char_position] + after[random_char_position + 1] \\\n",
    "            + after[random_char_position] + after[random_char_position + 2:])\n",
    "    return before, after\n",
    "\n",
    "def map_line_to_int(line, char2int_table):\n",
    "    encoded_seq = [char2int_table[char] for char in line]\n",
    "    str_array = map(lambda x: str(x), encoded_seq)\n",
    "    return \",\".join(str_array)\n",
    "\n",
    "def gen_X_y(char2int_table):\n",
    "    global clean_english_text_file, before_add_error_file, after_add_error_file\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_encoding = []\n",
    "    Y_encoding = []\n",
    "    before_file = open(before_add_error_file, \"w\")\n",
    "    after_file = open(after_add_error_file, \"w\")\n",
    "    change_index_fp = open(change_index_file, \"w\")\n",
    "    ori_file = open(clean_english_text_file, \"r\")\n",
    "    line_num = 1\n",
    "    for line in ori_file:\n",
    "        line = line.strip(\"\\n\")\n",
    "        correct, mistaken = add_error_to_line(line)\n",
    "        X.append([mistaken])\n",
    "        Y.append([correct])\n",
    "        before_file.write(correct + \"\\n\")\n",
    "        after_file.write(mistaken + \"\\n\")\n",
    "        if correct != mistaken:\n",
    "            change_index_fp.write(str(line_num) + \"\\n\")\n",
    "        line_num += 1\n",
    "        X_encoding.append(map_line_to_int(mistaken, char2int_table))\n",
    "        Y_encoding.append(map_line_to_int(correct, char2int_table))\n",
    "    pd_X = pd.DataFrame(X_encoding, columns=[\"text\"])\n",
    "    pd_Y = pd.DataFrame(Y_encoding, columns=[\"text\"])\n",
    "    return pd_X, pd_Y\n",
    "\n",
    "# DL要求将字符转成int作为输入\n",
    "def char2int(file=clean_english_text_file):\n",
    "    fp = open(file, \"r\")\n",
    "    all_text = fp.read()\n",
    "    fp.close()\n",
    "    chars = sorted(list(set(all_text)))\n",
    "    char2int_mapping =dict((c, i) for i, c in enumerate(chars))\n",
    "    return char2int_mapping, len(char2int_mapping)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract finish\n"
     ]
    }
   ],
   "source": [
    "# 1. 抽取SQuAD 2.0的问答数据\n",
    "json_data = read_squad_examples(\"./input_data/train-v2.0.json\", False)\n",
    "abstarct_sentence(json_data)\n",
    "clean_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '*': 11, '+': 12, ',': 13, '-': 14, '.': 15, '/': 16, '0': 17, '1': 18, '2': 19, '3': 20, '4': 21, '5': 22, '6': 23, '7': 24, '8': 25, '9': 26, ':': 27, ';': 28, '<': 29, '=': 30, '>': 31, '?': 32, '@': 33, 'A': 34, 'B': 35, 'C': 36, 'D': 37, 'E': 38, 'F': 39, 'G': 40, 'H': 41, 'I': 42, 'J': 43, 'K': 44, 'L': 45, 'M': 46, 'N': 47, 'O': 48, 'P': 49, 'Q': 50, 'R': 51, 'S': 52, 'T': 53, 'U': 54, 'V': 55, 'W': 56, 'X': 57, 'Y': 58, 'Z': 59, '[': 60, '\\\\': 61, ']': 62, '_': 63, '`': 64, 'a': 65, 'b': 66, 'c': 67, 'd': 68, 'e': 69, 'f': 70, 'g': 71, 'h': 72, 'i': 73, 'j': 74, 'k': 75, 'l': 76, 'm': 77, 'n': 78, 'o': 79, 'p': 80, 'q': 81, 'r': 82, 's': 83, 't': 84, 'u': 85, 'v': 86, 'w': 87, 'x': 88, 'y': 89, 'z': 90, '{': 91, '|': 92, '}': 93, '~': 94}\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "# 2. 生成码表\n",
    "char2int_table, table_size = char2int()\n",
    "print(char2int_table)\n",
    "print(table_size)\n",
    "\n",
    "# 2. 在抽取的文件中加入错误的噪音\n",
    "pd_X, pd_Y = gen_X_y(char2int_table)\n",
    "# 确保行数一样\n",
    "if len(pd_X.index) != len(pd_Y.index):\n",
    "    print(\"X and Y don't have same size\")\n",
    "    print(pd_X.info())\n",
    "    print(pd_Y.info())\n",
    "    sys.exit(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  56,72,69,78,1,68,73,68,1,35,69,89,79,78,67,69,...\n",
      "1    73,78,1,84,72,69,1,76,65,84,69,1,18,26,26,17,83\n",
      "2  56,72,65,84,1,65,82,69,65,83,1,68,73,68,1,35,6...\n",
      "3  83,73,78,71,73,78,71,1,65,78,68,1,68,65,78,67,...\n",
      "4  56,72,69,78,1,68,73,68,1,35,69,89,79,78,67,69,...\n",
      "                                                text\n",
      "0  56,72,69,78,1,68,73,68,1,35,69,89,79,78,67,69,...\n",
      "1    73,78,1,84,72,69,1,76,65,84,69,1,18,26,26,17,83\n",
      "2  56,72,65,84,1,65,82,69,65,83,1,68,73,68,1,35,6...\n",
      "3  83,73,78,71,73,78,71,1,65,78,68,1,68,65,78,67,...\n",
      "4  56,72,69,78,1,68,73,68,1,35,69,89,79,78,67,69,...\n"
     ]
    }
   ],
   "source": [
    "print(pd_X[0:5])\n",
    "print(pd_Y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 72, 69, 78, 1, 68, 73, 68, 1, 35, 69, 89, 79, 78, 67, 69, 1, 83, 84, 65, 82, 84, 1, 66, 69, 67, 79, 77, 73, 78, 71, 1, 80, 79, 80, 85, 76, 65, 82, 32]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "                                                text\n",
      "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n"
     ]
    }
   ],
   "source": [
    "# 要将Y向量转为one-hot来算softmax loss\n",
    "one_hot_pd_Y = []\n",
    "for row in pd_Y.iterrows():\n",
    "    row_transfer = []\n",
    "    array = row[1][\"text\"].split(\",\")\n",
    "    int_array = list(map(int, array))\n",
    "    transfer = to_categorical(int_array, num_classes=table_size)\n",
    "    row_transfer.append(transfer)\n",
    "    one_hot_pd_Y.append(row_transfer)\n",
    "    break\n",
    "\n",
    "pd_Y_transfer = pd.DataFrame(one_hot_pd_Y, columns=[\"text\"])\n",
    "print(pd_Y_transfer[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_char_model():\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # 输入序列变长，timestep设置为None.\n",
    "    # LSTM作为首层才需要设置input_shape参数。\n",
    "    model.add(LSTM(100, input_shape=(None, 1)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model1 = simple_char_model()\n",
    "model1.fit(pd_X, one_hot_pd_Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
