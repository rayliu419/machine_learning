{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract finish\n",
      "{'UNK': 1, ' ': 2, 'e': 3, 'o': 4, 'a': 5, 'i': 6, 'n': 7, 't': 8, 'h': 9, 'r': 10, 's': 11, 'd': 12, 'c': 13, 'l': 14, '\\n': 15, 'm': 16, 'w': 17, 'p': 18, 'y': 19, 'u': 20, 'f': 21, '?': 22, 'g': 23, 'W': 24, 'b': 25, 'B': 26, 'C': 27, 'v': 28, \"'\": 29, 'k': 30, '0': 31, '1': 32, 'S': 33, '2': 34, 'A': 35, 'H': 36, 'T': 37, 'F': 38, 'P': 39, ',': 40, 'M': 41, 'z': 42, 'L': 43, 'D': 44, '8': 45, 'I': 46, 'G': 47, '\"': 48, '.': 49, 'J': 50, 'R': 51, '3': 52, '4': 53, '5': 54, 'O': 55, 'x': 56, 'N': 57, 'E': 58, '-': 59, '9': 60, 'j': 61, 'V': 62, '6': 63, 'U': 64, '7': 65, 'q': 66, 'Z': 67, 'K': 68, 'Y': 69, '&': 70, '(': 71, ')': 72, 'X': 73, '/': 74, ':': 75, 'Q': 76, '$': 77}\n",
      "78\n",
      "When did Beyonce start becoming popular?\n",
      "When did Beyotce start becoming popular?\n",
      "(1494,)\n",
      "(1494,)\n",
      "non teacher forcing model\n",
      "(1494, 122, 78)\n"
     ]
    }
   ],
   "source": [
    "# 从SQuAD 2.0出去文本数据，加入随机噪音生成训练集。\n",
    "# 比较不同的model的纠错效果。\n",
    "# 英文数据\n",
    "# encoder-decoder 架构\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import choice as random_choice, randint as random_randint\n",
    "import sys\n",
    "import json\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Masking, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "english_text_file = \"./input_data/abstract_english_text_file\"\n",
    "clean_english_text_file = \"./input_data/clean_english_text_file\"\n",
    "before_add_error_file = \"./input_data/before_add_error\"\n",
    "after_add_error_file = \"./input_data/after_add_error\"\n",
    "change_index_file = \"./input_data/change_index_file\"\n",
    "MAX_LINE_NUMBER = 2000\n",
    "\n",
    "err_prob = {\n",
    "    \"replace_one_char\": 0.4,\n",
    "    \"add_one_char\": 0.2,\n",
    "    \"delete_one_char\": 0.2,\n",
    "    \"change_neighbor_order\": 0.2\n",
    "}\n",
    "\n",
    "# max_error_rate = 0.2\n",
    "char_list = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .\")\n",
    "max_error_line_length_rate = 0.05\n",
    "\n",
    "\n",
    "# 函数用来parse SQuAD 2.0的数据\n",
    "# 我们只需要抽取文本信息, SQuAD2.0的数据格式比较奇怪\n",
    "def read_squad_examples(input_file, is_training):\n",
    "    with tf.io.gfile.GFile(input_file, \"r\") as reader:\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "    return input_data\n",
    "\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# 抽取英文文本，中文和其他类别的去掉\n",
    "def abstarct_sentence(json_data):\n",
    "    global english_text_file\n",
    "    fp = open(english_text_file, \"w\")\n",
    "    for article in json_data:\n",
    "        paragraphs = article[\"paragraphs\"]\n",
    "        for paragraph in paragraphs:\n",
    "            qas = paragraph[\"qas\"]\n",
    "            for qa in qas:\n",
    "                question = qa[\"question\"]\n",
    "                if isEnglish(question):\n",
    "                    fp.write(question + \"\\n\")\n",
    "                answer_struct = qa[\"answers\"]\n",
    "                if (len(answer_struct) > 0):\n",
    "                    answer = answer_struct[0][\"text\"]\n",
    "                    if isEnglish(answer):\n",
    "                        fp.write(answer + \"\\n\")\n",
    "    fp.close()\n",
    "    print(\"abstract finish\")\n",
    "\n",
    "\n",
    "def clean_file(file=english_text_file):\n",
    "    global clean_english_text_file\n",
    "    fp = open(file, \"r\")\n",
    "    fp2 = open(clean_english_text_file, \"w\")\n",
    "    line_num = 0\n",
    "    for line in fp:\n",
    "        line = line.strip()\n",
    "        if line != \"\" and line != \"null\" and len(line) > 10:\n",
    "            fp2.write(line + \"\\n\")\n",
    "        line_num += 1\n",
    "        if (line_num == MAX_LINE_NUMBER):\n",
    "            break\n",
    "    fp.close()\n",
    "    fp2.close()\n",
    "\n",
    "\n",
    "def map_prob_to_range_with_keys(key_prob):\n",
    "    prob_list = list(key_prob.values())\n",
    "    prob_sum = sum(map(float,prob_list))\n",
    "    key_range = dict()\n",
    "    if not math.isclose(prob_sum, 1):\n",
    "        print(\"prob sum is not 1\")\n",
    "        sys.exit(-1)\n",
    "    else:\n",
    "        threshold = 0\n",
    "        for key, prob in key_prob.items():\n",
    "            key_range[key] = [threshold, threshold + prob]\n",
    "            threshold += prob\n",
    "    return key_range\n",
    "\n",
    "\n",
    "def choose_item_based_on_prob(key_range):\n",
    "    value = random.uniform(0, 1)\n",
    "    last_key = None\n",
    "    for key, prob_range in key_range.items():\n",
    "        last_key = key\n",
    "        if value >= prob_range[0] and value < prob_range[1]:\n",
    "            return key\n",
    "    return last_key\n",
    "\n",
    "\n",
    "# 随机修改正确的句子到错误的句子\n",
    "# 包括添加字符，删除字符，交换临近字符，替换字符\n",
    "def add_error_to_line(line):\n",
    "    max_error_line_length_rate\n",
    "    max_error_num = (int)(max_error_line_length_rate * len(line))\n",
    "    set_error_num = (int)(random.uniform(0, 1) * max_error_num)\n",
    "    cur_error_num = 0\n",
    "    before = line\n",
    "    after = line\n",
    "    key_range = map_prob_to_range_with_keys(err_prob)\n",
    "    while cur_error_num < set_error_num:\n",
    "        err_type = choose_item_based_on_prob(key_range)\n",
    "        cur_error_num += 1\n",
    "        if err_type == \"replace_one_char\":\n",
    "            random_char_position = random_randint(len(after))\n",
    "            after = after[:random_char_position] + random_choice(char_list[:-1]) \\\n",
    "                    + after[random_char_position + 1:]\n",
    "        elif err_type == \"add_one_char\":\n",
    "            random_char_position = random_randint(len(after))\n",
    "            after = after[:random_char_position] + random_choice(char_list[:-1]) \\\n",
    "                    + after[random_char_position:]\n",
    "        elif err_type == \"delete_one_char\":\n",
    "            random_char_position = random_randint(len(after))\n",
    "            after = after[:random_char_position] + after[random_char_position + 1:]\n",
    "        elif err_type == \"change_neighbor_order\":\n",
    "            random_char_position = random_randint(len(after) - 1)\n",
    "            after = (after[:random_char_position] + after[random_char_position + 1] \\\n",
    "                     + after[random_char_position] + after[random_char_position + 2:])\n",
    "    return before, after\n",
    "\n",
    "\n",
    "def gen_X_y(tk):\n",
    "    global clean_english_text_file, before_add_error_file, after_add_error_file\n",
    "    X = []\n",
    "    Y = []\n",
    "    X_encoding = []\n",
    "    Y_encoding = []\n",
    "    before_file = open(before_add_error_file, \"w\")\n",
    "    after_file = open(after_add_error_file, \"w\")\n",
    "    change_index_fp = open(change_index_file, \"w\")\n",
    "    ori_file = open(clean_english_text_file, \"r\")\n",
    "    line_num = 1\n",
    "    print_test = True\n",
    "    for line in ori_file:\n",
    "        line = line.strip(\"\\n\")\n",
    "        correct, mistaken = add_error_to_line(line)\n",
    "        X.append([mistaken])\n",
    "        Y.append([correct])\n",
    "        before_file.write(correct + \"\\n\")\n",
    "        after_file.write(mistaken + \"\\n\")\n",
    "        if correct != mistaken:\n",
    "            change_index_fp.write(str(line_num) + \"\\n\")\n",
    "            if print_test:\n",
    "                print(correct)\n",
    "                print(mistaken)\n",
    "            print_test = False\n",
    "        line_num += 1\n",
    "        X_encoding.append(tk.texts_to_sequences([mistaken])[0])\n",
    "        Y_encoding.append(tk.texts_to_sequences([correct])[0])\n",
    "    np_array_X = np.array([np.array(xi) for xi in X_encoding])\n",
    "    np_array_Y = np.array([np.array(yi) for yi in Y_encoding])\n",
    "    # np_array_X 是错的句子\n",
    "    # np_array_Y 是对的句子\n",
    "    return np_array_X, np_array_Y\n",
    "\n",
    "\n",
    "# DL要求将字符转成int作为输入\n",
    "def char2int(file=clean_english_text_file):\n",
    "    fp = open(file, \"r\")\n",
    "    all_text = fp.read()\n",
    "    fp.close()\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token=\"UNK\", lower=False)\n",
    "    tk.fit_on_texts(all_text)\n",
    "    return tk\n",
    "\n",
    "\n",
    "# pad 2d 数组\n",
    "def pad_sequence(array_2d, arg_max_len=None):\n",
    "    max_len = max(len(a) for a in array_2d)\n",
    "    if arg_max_len:\n",
    "        max_len = arg_max_len\n",
    "    return pad_sequences(array_2d, padding=\"post\", maxlen=max_len)\n",
    "\n",
    "\n",
    "def prepare_test_sentences(tk, sentences, max_length, char_table_size):\n",
    "    sentences_int = tk.texts_to_sequences(sentences)\n",
    "    sample_num = len(sentences_int)\n",
    "    np_array = np.array([np.array(xi) for xi in sentences_int])\n",
    "    sentences_int_pad_one_hot = one_hot_encode_2d_array(np_array, sample_num, True, max_length, char_table_size)\n",
    "    return sentences_int_pad_one_hot\n",
    "\n",
    "\n",
    "def get_dict_index_back(ndarray):\n",
    "    result = []\n",
    "    for row in range(0, ndarray.shape[1]):\n",
    "        mapping_int = np.argmax(ndarray[0][row])\n",
    "        result.append(mapping_int)\n",
    "    return result\n",
    "\n",
    "\n",
    "def reverse_dict(tk, int_array):\n",
    "    char_result = []\n",
    "    inverse_dict = {v: k for k, v in tk.word_index.items()}\n",
    "    for dict_index in int_array:\n",
    "        if dict_index in inverse_dict:\n",
    "            char_result.append(inverse_dict[dict_index])\n",
    "        else:\n",
    "            char_result.append(\" \")\n",
    "    return char_result\n",
    "\n",
    "\n",
    "def reset_zero(one_hot):\n",
    "    find_EOS = False\n",
    "    for i in range(one_hot.shape[0]):\n",
    "        for j in range(one_hot.shape[1]):\n",
    "            if j == 0 and one_hot[i][j] == 1:\n",
    "                if find_EOS:\n",
    "                    one_hot[i][j] = 0\n",
    "                else:\n",
    "                    find_EOS = True\n",
    "                break\n",
    "\n",
    "\n",
    "# 将2d转成one_hot的3d\n",
    "def one_hot_encode_2d_array(array_2d, sample_num, need_pad, pad_length, code_table_size, pad_clear=True):\n",
    "    np_pad = array_2d\n",
    "    if need_pad:\n",
    "        np_pad = pad_sequence(array_2d, pad_length)\n",
    "    np_pad_one_hot = np.empty(shape=(sample_num, pad_length, code_table_size))\n",
    "    for index in range(0, sample_num):\n",
    "        cur_one_hot = to_categorical(np_pad[index], num_classes=code_table_size)\n",
    "        if pad_clear:\n",
    "            reset_zero(cur_one_hot)\n",
    "        np_pad_one_hot[index] = cur_one_hot\n",
    "    print(np_pad_one_hot.shape)\n",
    "    return np_pad_one_hot\n",
    "\n",
    "\n",
    "def save_model_history_to_file(history, filename):\n",
    "    hist_df = pd.DataFrame.from_dict(history.history)\n",
    "    # save to json:\n",
    "    hist_json_file = filename\n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "\n",
    "\n",
    "def train_wrong_one_hot_simple_char_model(np_pad_X, np_pad_Y, pad_max_length, char_table_size):\n",
    "    \"\"\" 使用自编码的one_hot来作为输入和输出\n",
    "    这种模型不是真的有效果，主要是填充的值用什么表示的问题，我尝试过几种方法:\n",
    "    1. 默认用0的to_categorical，会在0位置1。 - 由于大量的这种数据存在，直观上来说会导致预测都向空字符。\n",
    "    2. 0->0 向量，也不行。\n",
    "    3. 第一个结束用[1,0,0...]，后续用[0,0,0]表示。- 有些例子中这样做，但是我用了还是不行。\n",
    "    然而都不行。\n",
    "    所以在输入层还是必须要显式的告诉哪些是有用的。\n",
    "    \"\"\"\n",
    "    sample_num = np_pad_Y.shape[0]\n",
    "    np_pad_one_hot_Y = one_hot_encode_2d_array(np_pad_Y, sample_num, False, pad_max_length, char_table_size, True)\n",
    "    np_pad_one_hot_X = one_hot_encode_2d_array(np_pad_X, sample_num, False, pad_max_length, char_table_size, True)\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # LSTM作为首层才需要设置input_shape参数。\n",
    "    model.add(LSTM(20, input_shape=(pad_max_length, char_table_size), return_sequences=True))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dense(char_table_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    model.fit(np_pad_one_hot_X, np_pad_one_hot_Y, epochs=10, verbose=1, batch_size=64)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_simple_embedding_model(max_len, feature_num, output_unit, np_pad_X, np_pad_Y):\n",
    "    \"\"\"\n",
    "    这并不是传统的seq2seq，因为decoder的输入Y_predict(i+1)不是根据Y_predict(i)和decoder的state计算的。\n",
    "    加入了masking，并且使用Embedding层来编码，效果训练1小时能达到60%\n",
    "    这是一种自己创建的方法，看起来也是有效果的。\n",
    "    \"\"\"\n",
    "    char_table_size = feature_num\n",
    "    sample_num = np_pad_Y.shape[0]\n",
    "    np_pad_one_hot_Y = one_hot_encode_2d_array(np_pad_Y, sample_num, False, max_len, char_table_size, False)\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=feature_num, input_length=max_len, output_dim=10, mask_zero=True))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dense(output_unit, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    history = model.fit(np_pad_X, np_pad_one_hot_Y, epochs=10, verbose=1, batch_size=64)\n",
    "    save_model_history_to_file(history, \"simple_mapping.csv\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_non_teacher_forcing_model(max_len, feature_num, output_unit, np_pad_X, np_pad_Y):\n",
    "    \"\"\"\n",
    "    decoder的输出是一次一个timestep，因为LSTM的新的预测是基于上一个output来做的。\n",
    "    完全train不动，感觉是哪里写的有问题。但是框架大概是这样的\n",
    "    \"\"\"\n",
    "    # 为实参输入做准备\n",
    "    max_len_adjust = max_len + 1\n",
    "    np_encoder_input_pad = pad_sequences(np_pad_X, padding=\"post\", maxlen=max_len_adjust)\n",
    "    np_dense_pad = pad_sequences(np_pad_Y, padding=\"post\", maxlen=max_len_adjust)\n",
    "    sample_num = np_dense_pad.shape[0]\n",
    "    np_dense_pad_one_hot_Y = one_hot_encode_2d_array(np_dense_pad, sample_num, False, max_len_adjust, char_table_size)\n",
    "    decoder_input_data = np.zeros((sample_num, 1, feature_num))\n",
    "\n",
    "    embedding_input = Input(shape=(max_len_adjust, ))\n",
    "    embedding_layer = Embedding(input_dim=feature_num, input_length=max_len_adjust, output_dim=10)\n",
    "    embedding_output = embedding_layer(embedding_input)\n",
    "\n",
    "    encoder_layer = LSTM(20, return_state=True)\n",
    "    encoder_output, encoder_last_output, encoder_last_state = encoder_layer(embedding_output)\n",
    "    states = [encoder_last_output, encoder_last_state]\n",
    "\n",
    "    decoder_input = Input(shape=(1, feature_num))\n",
    "    decoder_layer = LSTM(20, return_sequences=True, return_state=True)\n",
    "    dense_layer = Dense(output_unit,  activation=\"softmax\")\n",
    "\n",
    "    all_output = []\n",
    "    cur_input = decoder_input\n",
    "    for _ in range(max_len_adjust):\n",
    "        decoder_output, decoder_last_output, decoder_last_state = decoder_layer(cur_input, initial_state=states)\n",
    "        dense_output = dense_layer(decoder_output)\n",
    "        all_output.append(dense_output)\n",
    "        cur_input = dense_output\n",
    "        states = [decoder_last_output, decoder_last_state]\n",
    "    predict_outputs = keras.backend.concatenate(all_output, axis=1)\n",
    "    model = Model([embedding_input, decoder_input], predict_outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    # model = Model([embedding_input, decoder_input], predict_outputs)\n",
    "    # model.fit([np_encoder_input_pad, decoder_input_data], np_dense_pad_one_hot_Y) 比较\n",
    "    model.fit([np_encoder_input_pad, decoder_input_data], np_dense_pad_one_hot_Y, epochs=10, verbose=1, batch_size=64)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_teacher_forcing_model(max_len, feature_num, output_unit, np_pad_X, np_pad_Y):\n",
    "    \"\"\"\n",
    "    使用Function APi来定义teacher forcing model.\n",
    "    1. 先embedding.\n",
    "    2. 将embedding的output传给encoder LSTM, 在teacher forcing model下，只需要last state\n",
    "    3. decoder的input跟simple embedding的输入不一样。\n",
    "    simple embedding的序列Y_predict(i+1)是根据Y_predict(i)和decoder的state计算的。\n",
    "    teacher forcing modelY_predict(i+1)是根据Y_actual(i)和decoder的state计算的。\n",
    "    从效果上来说，还不如simple_mapping...\n",
    "    TODO: 可以不适用Embedding的来train，而是使用gensim的pre-train的向量看看\n",
    "    \"\"\"\n",
    "    # 为teacher_forcing fit model准备实参\n",
    "    max_len_adjust = max_len + 1\n",
    "    np_encoder_input_pad = pad_sequences(np_pad_X, padding=\"post\", maxlen=max_len_adjust)\n",
    "    np_decoder_input_pad = pad_sequences(np_pad_Y, padding=\"pre\", maxlen=max_len_adjust)\n",
    "    np_dense_pad = pad_sequences(np_pad_Y, padding=\"post\", maxlen=max_len_adjust)\n",
    "    sample_num = np_dense_pad.shape[0]\n",
    "    np_dense_pad_one_hot_Y = one_hot_encode_2d_array(np_dense_pad, sample_num, False, max_len_adjust, char_table_size)\n",
    "\n",
    "    # 定义encoder网络\n",
    "    # 接受一个max_len长度的sequence的int\n",
    "    encoder_input_ph = Input(shape=(max_len_adjust, ))\n",
    "    encoder_embedding_layer = Embedding(input_dim=feature_num, input_length=max_len_adjust, output_dim=10, mask_zero=True)\n",
    "    embedding_output_ph = encoder_embedding_layer(encoder_input_ph)\n",
    "    encoder_LSTM_layer = LSTM(20, return_state=True)\n",
    "    _, encoder_last_output, encoder_last_state = encoder_LSTM_layer(embedding_output_ph)\n",
    "    decoder_initial_state = [encoder_last_output, encoder_last_state]\n",
    "\n",
    "    # 定义decoder网络\n",
    "    decoder_input_ph = Input(shape=(max_len_adjust, ))\n",
    "    # 设置return_state是为了inference\n",
    "    # 通过initial_state将encoder和decoder连接在了一起\n",
    "    decoder_embedding_layer = Embedding(input_dim=feature_num, input_length=max_len_adjust, output_dim=10, mask_zero=True)\n",
    "    embedding_output_ph2 = decoder_embedding_layer(decoder_input_ph)\n",
    "\n",
    "    decoder_LSTM_layer = LSTM(20, return_sequences=True, return_state=True)\n",
    "    decoder_output_ph, _, _ = decoder_LSTM_layer(embedding_output_ph2, initial_state=decoder_initial_state)\n",
    "    dense_layer = Dense(output_unit, activation=\"softmax\")\n",
    "    final_dense_output = dense_layer(decoder_output_ph)\n",
    "\n",
    "    model = Model([encoder_input_ph, decoder_input_ph], final_dense_output)\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    # model = Model([encoder_input_ph, decoder_input_ph], final_dense_output)\n",
    "    # model.fit([np_encoder_input_pad, np_decoder_input_pad], np_dense_pad_one_hot_Y)\n",
    "    # 合起来看，第一行其实是用占位符描述了, 但是最后的参数 final_dense_output 和 np_pad_one_hot_Y不一样。\n",
    "    history = model.fit([np_encoder_input_pad, np_decoder_input_pad], np_dense_pad_one_hot_Y, epochs=10, verbose=1, batch_size=64)\n",
    "    save_model_history_to_file(history, \"teacher_forcing.csv\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_one_hot_simple_model(model, tk, sentences, max_length, char_table_size):\n",
    "    test_np_pad_one_hot_X = prepare_test_sentences(tk, sentences, max_length, char_table_size)\n",
    "    # print(test_np_pad_one_hot_X)\n",
    "    predict_Y = model.predict(test_np_pad_one_hot_X, verbose=1)\n",
    "    predict_Y_int = get_dict_index_back(predict_Y)\n",
    "    final_result = reverse_dict(tk, predict_Y_int)\n",
    "    print(\"input======================================================================\")\n",
    "    print(sentences)\n",
    "    print(test_np_pad_one_hot_X)\n",
    "    print(\"after======================================================================\")\n",
    "    print(predict_Y_int)\n",
    "    predict_sentence = \"\".join(final_result)\n",
    "    print(predict_sentence)\n",
    "\n",
    "\n",
    "def test_simple_embedding_model(model, tk, sentences, max_length):\n",
    "    sentences_int = tk.texts_to_sequences(sentences)\n",
    "    sentences_int_pad = pad_sequences(sentences_int, padding='post', maxlen=max_length)\n",
    "    predict_Y = model.predict(sentences_int_pad, verbose=1)\n",
    "    predict_Y_int = get_dict_index_back(predict_Y)\n",
    "    final_result = reverse_dict(tk, predict_Y_int)\n",
    "    print(\"input======================================================================\")\n",
    "    print(sentences)\n",
    "    print(sentences_int)\n",
    "    print(\"after======================================================================\")\n",
    "    print(predict_Y_int)\n",
    "    predict_sentence = \"\".join(final_result)\n",
    "    print(predict_sentence)\n",
    "\n",
    "\n",
    "def test_teacher_forcing_model():\n",
    "    return\n",
    "\n",
    "\n",
    "def test_attention_model():\n",
    "    return\n",
    "\n",
    "\n",
    "def test_beam_search_mode():\n",
    "    return\n",
    "\n",
    "\n",
    "def test_bi_LSTM_mode():\n",
    "    return\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"growing upY\"\n",
    "]\n",
    "\n",
    "\n",
    "# 1. 抽取SQuAD 2.0的问答数据\n",
    "json_data = read_squad_examples(\"./input_data/train-v2.0.json\", False)\n",
    "abstarct_sentence(json_data)\n",
    "clean_file()\n",
    "\n",
    "# 2. 生成码表\n",
    "tk = char2int()\n",
    "print(tk.word_index)\n",
    "char_table_size = len(tk.word_index) + 1\n",
    "print(char_table_size)\n",
    "\n",
    "# 3. 在抽取的文件中加入错误的噪音\n",
    "np_array_X, np_array_Y = gen_X_y(tk)\n",
    "# 确保行数一样\n",
    "print(np_array_X.shape)\n",
    "print(np_array_Y.shape)\n",
    "\n",
    "# 4. padding\n",
    "max_X_length = max(len(xi) for xi in np_array_X)\n",
    "max_Y_length = max(len(yi) for yi in np_array_Y)\n",
    "max_length = max(max_X_length, max_Y_length)\n",
    "np_pad_X = pad_sequences(np_array_X, padding='post', maxlen=max_length)\n",
    "np_pad_Y = pad_sequences(np_array_Y, padding='post', maxlen=max_length)\n",
    "\n",
    "# one hot model\n",
    "# print(\"one hot model\")\n",
    "# self_one_hot_model = train_wrong_one_hot_simple_char_model(np_pad_X, np_pad_Y, max_length, char_table_size)\n",
    "# test_one_hot_simple_model(self_one_hot_model, tk, sentences, max_length, char_table_size)\n",
    "\n",
    "# # embedding model\n",
    "# print(\"simple embedding model\")\n",
    "# simple_embedding_model = train_simple_embedding_model(max_length, char_table_size, char_table_size, np_pad_X, np_pad_Y)\n",
    "# test_simple_embedding_model(simple_embedding_model, tk, sentences, max_length)\n",
    "\n",
    "# # teacher forcing model\n",
    "# print(\"teacher forcing model\")\n",
    "# teacher_forcing_model = train_teacher_forcing_model(max_length, char_table_size, char_table_size, np_pad_X, np_pad_Y)\n",
    "\n",
    "# non teacher forcing model - 训练不动，考虑使用pre-train的向量试试\n",
    "print(\"non teacher forcing model\")\n",
    "non_teacher_forcing_model = train_non_teacher_forcing_model(max_length, char_table_size, char_table_size, np_pad_X, np_pad_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
