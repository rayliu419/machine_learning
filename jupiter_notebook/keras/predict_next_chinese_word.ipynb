{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测下一个汉字。 \n",
    "# 1. one word in, one word out \n",
    "# 2. multiple word in, one word out.\n",
    "# 通过model的性能可以看出来，two word in 优于 one word in\n",
    "# 光使用10000条公交站数据，one word能到60%，two word能到66%。使用普通数据10000条，one word 26%, two word 34%，\n",
    "# 说明ngram确实是work的，另外，提高epoch和增加训练数据可以提升。\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Embedding\n",
    "import sys\n",
    "sys.path.append(\"../common/\")\n",
    "import chinese_preprocess\n",
    "import probability_utils\n",
    "import chinese_poi_helper\n",
    "import nlp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topn_prob_word(y_distribution, tokenizer):\n",
    "    \"\"\"\n",
    "    获取概率最高的字和相应的概率\n",
    "    :param y_distribution:\n",
    "    :param tokenizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    prob_array = y_distribution[0]\n",
    "    _, indexes = probability_utils.top_k_values_and_indexes(prob_array)\n",
    "    result = probability_utils.map_indexes_to_word(indexes, tokenizer, prob_array)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predict_one_word_with_prob(model, tokenizer, word, length=1):\n",
    "    \"\"\"\n",
    "    获取模型的输出，映射到字并给出概率\n",
    "    :param model:\n",
    "    :param tokenizer:\n",
    "    :param word:\n",
    "    :param length:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    in_text = word\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = encoded[-length:]\n",
    "    encoded_np = np.array(encoded)\n",
    "    encoded_np = encoded_np[np.newaxis, :]\n",
    "    y_distribution = model.predict(encoded_np, verbose=0)\n",
    "    topn_word = find_topn_prob_word(y_distribution, tokenizer)\n",
    "    print(\"input - {}\".format(word))\n",
    "    print(\"predict - \")\n",
    "    print(topn_word)\n",
    "    return topn_word\n",
    "\n",
    "\n",
    "def generate_seq_with_one_word(model, tokenizer, seed_text, n_words):\n",
    "    \"\"\"\n",
    "    从一个word开始预测序列，实际中预测的后面的词就不太靠谱了\n",
    "    :param model:\n",
    "    :param tokenizer:\n",
    "    :param seed_text:\n",
    "    :param n_words:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    in_text, result = seed_text, seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        (out_word, probability) = predict_one_word_with_prob(model, tokenizer, in_text)[0]\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_seq_with_multiple_word(model, tokenizer, seed_text, n_words, length):\n",
    "    \"\"\"\n",
    "    从多个word预测序列，实际中预测的后面的词就不太靠谱了\n",
    "    :param model:\n",
    "    :param tokenizer:\n",
    "    :param seed_text:\n",
    "    :param n_words:\n",
    "    :param length:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    in_text, result = seed_text, seed_text\n",
    "    for x in range(n_words):\n",
    "        (out_word, probability) = predict_one_word_with_prob(model, tokenizer, in_text, length)[0]\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "        # 改变输入变量\n",
    "        encoded[0].pop(0)\n",
    "        new_encode = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded[0].extend(new_encode)\n",
    "    return result\n",
    "\n",
    "\n",
    "def gen_X_y(encoded, length=1):\n",
    "    word_seq = nlp_utils.generate_ngram_multiple(encoded, length)\n",
    "    sequences = np.array(word_seq)\n",
    "    # 生成X = [word_int1, word_int2, ...] -> y = word_int\n",
    "    X, y = sequences[:, 0:-1], sequences[:, -1]\n",
    "    # 为什么是one hot的方式，因为最后的输出也是one hot。\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_and_train_model(vocab_size, length, X, y):\n",
    "    \"\"\"\n",
    "    从RNN(LSTM)的角度来看，多个字input，就是hidden的信息多了东西。\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=length))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=20, verbose=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"#1 load data\")\n",
    "chinese_poi_name = chinese_poi_helper.prepare_chinese_poi_raw_data_for_task(True)\n",
    "print(chinese_poi_name[0:2])\n",
    "print(\"#2 transform to int\")\n",
    "tokenizer, encoded, vocab_size = chinese_preprocess.encode_chinese_to_int(chinese_poi_name)\n",
    "print(encoded[0:2])\n",
    "print(\"vocab_size : {}\".format(vocab_size))\n",
    "print(chinese_poi_name[0])\n",
    "print(tokenizer.texts_to_sequences([chinese_poi_name[0]]))\n",
    "\n",
    "length_one = 1\n",
    "print(\"#3 train with {}\".format(length_one))\n",
    "X1, y1 = gen_X_y(encoded, length_one)\n",
    "model1 = build_and_train_model(vocab_size, length_one, X1, y1)\n",
    "model1.summary()\n",
    "# 测试单个字输入的预测\n",
    "predict_one_word_with_prob(model1, tokenizer, \"公\")\n",
    "predict_one_word_with_prob(model1, tokenizer, \"机\")\n",
    "predict_one_word_with_prob(model1, tokenizer, \"阳\")\n",
    "# 测试单个字输入预测序列的效果\n",
    "test_word1 = ['公', '机', '阳']\n",
    "for word in test_word1:\n",
    "    print(word)\n",
    "    print(generate_seq_with_one_word(model1, tokenizer, word, 1))\n",
    "\n",
    "\n",
    "length_two = 2\n",
    "X2, y2 = gen_X_y(encoded, length_two)\n",
    "model2 = build_and_train_model(vocab_size, length_two, X2, y2)\n",
    "model2.summary()\n",
    "# 测试两个字预测一个字\n",
    "predict_one_word_with_prob(model2, tokenizer, chinese_preprocess.seg_chinese_single(\"公安\"), 2)\n",
    "predict_one_word_with_prob(model2, tokenizer, \"机 关\", 2)\n",
    "predict_one_word_with_prob(model2, tokenizer, \"阳 光\", 2)\n",
    "# 测试两个字预测序列\n",
    "test_word2 = ['公 安', '机 关', '阳 光']\n",
    "for word in test_word2:\n",
    "    print(word)\n",
    "    print(generate_seq_with_multiple_word(model2, tokenizer, word, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
