{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测下一个汉字。 \n",
    "# 1. one word in, one word out \n",
    "# 2. mulitple word in, one word out.\n",
    "# 通过model的性能可以看出来，two word in 优于 one word in\n",
    "# 光使用10000条公交站数据，one word能到60%，two word能到66%。使用普通数据10000条，one word 26%, two word 34%，\n",
    "# 说明ngram确实是work的，另外，提高epoch和增加训练数据可以提升。\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from tensorflow.python.keras import backend as k\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Embedding\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append(\"../common/\")\n",
    "import chinese_preprocess\n",
    "\n",
    "\n",
    "def find_top_n_in_prob_array(prob_array, topn=5):\n",
    "    idx = (-prob_array).argsort()[:topn]\n",
    "    return idx\n",
    "    \n",
    "def map_top_n_int_to_word(index_array, tokenizer, y_distribution):\n",
    "    int_set = set(index_array)\n",
    "    result = []\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index in int_set:\n",
    "            result.append((word, y_distribution[index]))\n",
    "    return result\n",
    "\n",
    "def find_topn_prob_word(y_distribution, model, tokenizer):\n",
    "    prob_array = y_distribution[0]\n",
    "    idx = find_top_n_in_prob_array(prob_array)\n",
    "    # print(idx)\n",
    "    topn_word = map_top_n_int_to_word(idx, tokenizer, prob_array)\n",
    "    topn_word.sort(key=lambda x: x[1], reverse=True)\n",
    "    return topn_word\n",
    "\n",
    "# 还输出各个字的预测概率\n",
    "def generate_one_word_with_prob(model, tokenizer, word, length=1):\n",
    "    in_text = word\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = encoded[-length:]\n",
    "    encoded = [encoded]\n",
    "    y_distribution = model.predict(encoded, verbose=0)\n",
    "    print(\"input {}\".format(word))\n",
    "    topn_word = find_topn_prob_word(y_distribution, model, tokenizer)\n",
    "    print(topn_word)\n",
    " \n",
    "def map_yaht_to_word(yhat, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == yhat:\n",
    "            return word\n",
    "    return ''\n",
    "\n",
    "# 实际上可以连续预测多个词\n",
    "# 实际中预测的后面的词就不太靠谱了 \n",
    "def generate_seq_with_one_word(model, tokenizer, seed_text, n_words):\n",
    "    in_text, result = seed_text, seed_text\n",
    "        # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = array(encoded)\n",
    "        # predict a word in the vocabulary\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        y_distribution = model.predict(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "    return result\n",
    "\n",
    "# 实际上可以连续预测多个词\n",
    "def generate_seq_with_multiple_word(model, tokenizer, seed_text, n_words, length):\n",
    "    in_text, result = seed_text, seed_text\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = encoded[-length:]\n",
    "    seed_text_length = len(encoded)\n",
    "    encoded = [encoded]\n",
    "    if (seed_text_length < length):\n",
    "        print(\"context length is smaller than required length\")\n",
    "        return\n",
    "    for x in range(n_words):\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # 输出的class映射回word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text, result = out_word, result + ' ' + out_word\n",
    "        # 改变输入变量\n",
    "        encoded[0].pop(0)\n",
    "        new_encode = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded[0].extend(new_encode)\n",
    "    return result\n",
    "\n",
    "# 构造one-word-in, one-word-out的神经网络的输入\n",
    "def generate_word_pair(encoded):\n",
    "    sequences = list()\n",
    "    for i in encoded:\n",
    "        tuple_temp = tuple(i)\n",
    "        for pair in list(zip(i, i[1:])):\n",
    "            sequences.append(pair)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# 构造多个word的神经网络输入\n",
    "# encoded = [[1,2,3], [4,5,6]] length = 2\n",
    "# [1,2] [2,3] [4,5] [5,6]\n",
    "def generate_multiple_word_seq(encoded, length=1):\n",
    "    sequences = list()\n",
    "    for array in encoded:\n",
    "        # [95, 4, 233, 37, 2, 3, 1]\n",
    "        for idx, val in enumerate(array):\n",
    "            seq = []\n",
    "            if idx + length < len(array):\n",
    "                seq = array[idx:idx + length + 1]\n",
    "                sequences.append(seq) \n",
    "            else:\n",
    "                break\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences    \n",
    "\n",
    "# 生成X = [word_int1, word_int2, ...] -> word_int\n",
    "def gen_X_y(encoded, length=1):\n",
    "    word_seq = generate_multiple_word_seq(encoded, length)\n",
    "    sequences = array(word_seq)\n",
    "    X, y = sequences[:,0:-1],sequences[:,-1]\n",
    "    # 为什么是one hot的方式\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    return X, y\n",
    "\n",
    "def build_and_train_model(vocab_size, length, X, y):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10, input_length=length))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=20, verbose=1)\n",
    "    return model\n",
    "\n",
    "def divide_chunks(l, n): \n",
    "    # looping till length l \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理是一样的\n",
    "chinese_poi_name = chinese_preprocess.load_chinese_and_seg(\"./input_data/test_10000\")\n",
    "print(chinese_poi_name[0:2])\n",
    "tokenizer, encoded, vocab_size = chinese_preprocess.encode_chinese_to_int(chinese_poi_name)\n",
    "print(encoded[0:2])\n",
    "print(\"vocab_size : {}\".format(vocab_size))\n",
    "print(chinese_poi_name[0])\n",
    "print(tokenizer.texts_to_sequences([chinese_poi_name[0]]))\n",
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_one = 1\n",
    "X1, y1 = gen_X_y(encoded, length_one)\n",
    "model1 = build_and_train_model(vocab_size, length_one, X1, y1)\n",
    "\n",
    "generate_one_word_with_prob(model1, tokenizer, \"公\")\n",
    "generate_one_word_with_prob(model1, tokenizer, \"机\")\n",
    "generate_one_word_with_prob(model1, tokenizer, \"阳\")\n",
    "\n",
    "# 测试效果 one word in\n",
    "test_word1 = []\n",
    "test_word1.append(\"公\")\n",
    "test_word1.append(\"机\")\n",
    "test_word1.append(\"阳\")\n",
    "for word in test_word1:\n",
    "    print(word)\n",
    "    print(generate_seq_with_one_word(model1, tokenizer, word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_two = 2\n",
    "X2, y2 = gen_X_y(encoded, length_two)\n",
    "model2 = build_and_train_model(vocab_size, length_two, X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.texts_to_sequences([\"公 安\"]))\n",
    "\n",
    "generate_one_word_with_prob(model2, tokenizer, \"公 安\", 2)\n",
    "generate_one_word_with_prob(model2, tokenizer, \"机 关\", 2)\n",
    "generate_one_word_with_prob(model2, tokenizer, \"阳 光\", 2)\n",
    "\n",
    "\n",
    "# 测试效果 multiple word in \n",
    "test_word2 = []\n",
    "test_word2.append(\"公 安\")\n",
    "test_word2.append(\"机 关\")\n",
    "test_word2.append(\"阳 光\")\n",
    "\n",
    "for word in test_word2:\n",
    "    print(word)\n",
    "    print(generate_seq_with_multiple_word(model2, tokenizer, word, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_details(model):\n",
    "    model.summary()\n",
    "    model.get_config()\n",
    "    \n",
    "def model_layer_information(model, layer_index):\n",
    "    print(model.layers[layer_index].input_shape)\n",
    "    print(model.layers[layer_index].output_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
