{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "情感预测的练习。对于这个练习，\n",
    "对于精度有影响的：\n",
    "    1.增加训练集。5000->10000，精度提升10%。\n",
    "    2.增加模型复杂度。simple->LSTM，更重要的是simple容易过拟合。\n",
    "    3.stopword在此问题中不应该去除，会降低精度。主要原因应该是shouldn't之类可以判断情感的词被过滤掉了。\n",
    "    4.增加epoch的次数。\n",
    "减小过拟合的方法：\n",
    "    1.加入dropout。\n",
    "    2.降低模型复杂度。LSTM单元数减少。\n",
    "    3.增加训练集。\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, LSTM, Bidirectional, Input \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import regularizers\n",
    "import sys\n",
    "sys.path.append(\"../common/\")\n",
    "import english_preprocess\n",
    "import movie_review_helper\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type, file_name):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(file_name)       \n",
    "\n",
    "def predict_single(review, model):\n",
    "    review = tokenizer.texts_to_sequences(review)\n",
    "    flat_list = []\n",
    "    for sublist in review:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    flat_list = [flat_list]\n",
    "    review = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n",
    "    print(model.predict(review))\n",
    "\n",
    "tokenizer, embedding_mapping, X, Y, X_train, X_test, Y_train, Y_test = movie_review_helper.prepare_movie_review_for_task(10000, 100, 100)\n",
    "maxlen = 100\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    return dt_string\n",
    "\n",
    "def epoch_performance(history, model_name):\n",
    "    print(\"model name : {}\".format(model_name))\n",
    "    for i in history.epoch:\n",
    "        print(\"epoch {}\".format(i))\n",
    "        print(\"train acc : {} validate acc : {}\".format(history.history[\"acc\"][i], history.history[\"val_acc\"][i]))\n",
    "        print(\"train loss : {} validate loss : {}\".format(history.history[\"loss\"][i], history.history[\"val_loss\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model = Sequential()\n",
    "    \"\"\"\n",
    "    All that the Embedding layer does is to map the integer inputs to the vectors found at the\n",
    "    corresponding index in the embedding matrix,\n",
    "    i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "    这种模型overfitting比较严重。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Flatten())\n",
    "    # 光通过这个解决不了overfitting的问题\n",
    "    # model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file , show_shapes=True)\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    epoch_performance(history, model_name)\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training simple model\")\n",
    "model1 = simple_model(\"simple_model\" ,vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_many_to_one_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model\n",
    "\n",
    "def LSTM_many_to_one_model_v2(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    降低模型的overfit。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    # 减少LSTM的个数\n",
    "    model.add(LSTM(32))\n",
    "    # 加入dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"start training LSTM v1 model\")\n",
    "# model3 = LSTM_many_to_one_model(\"LSTM_v1\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "# 能达到90%多\n",
    "print(\"start training LSTM v2 model\")\n",
    "model4 = LSTM_many_to_one_model_v2(\"LSTM_v2\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BI_LSTM_many_to_one_model(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    降低模型的overfit。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training BI_LSTM model\")\n",
    "model5 = BI_LSTM_many_to_one_model(\"BI_LSTM_v1\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_many_to_one_model_v3(model_name, dict_size, embedding_matrix, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    变长input，但是实际上通过不了。至少在tensorflow 1.15版本是不行的。\n",
    "    无论是Sequential()还是function API。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    \n",
    "    embedding_input = Input(shape=(None,))\n",
    "    embedding_output = Embedding(dict_size, 100, weights=[embedding_matrix], trainable=False, input_length=None)(embedding_input)\n",
    "    LSTM_output = LSTM(32)(embedding_output)\n",
    "    dropout_output = Dropout(0.5)(LSTM_output)\n",
    "    dense_output = Dense(1, activation='sigmoid')(dropout_output)\n",
    "    \n",
    "    model = Model(embedding_input, dense_output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "        \n",
    "    history_plot = LossHistory()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=[history_plot])\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际上无法运行\n",
    "# print(\"start training LSTM v3 model\")\n",
    "# model6 = LSTM_many_to_one_model_v3(\"LSTM_v3\", vocab_size, embedding_mapping, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_callbacks(model_save_file):\n",
    "    \"\"\"\n",
    "    加入early stopping\n",
    "    降低learning rate\n",
    "    \"\"\"\n",
    "    # save model callback\n",
    "    checkpoint = ModelCheckpoint(model_save_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    # early stop callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "    # reduce learning rate\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1)\n",
    "    return [checkpoint, early_stopping, reduce_lr]\n",
    "    \n",
    "    \n",
    "def LSTM_many_to_one_model_v4(model_name, dict_size, embedding_matrix, maxlen, X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"\n",
    "    当前最高的val_acc是86%左右，想办法继续优化LSTM_many_to_one_model_v2\n",
    "    1.尝试加入更多的dense+relu，两层dropout - done\n",
    "    2.dense加入egularizer。 - done\n",
    "    3.加入early stopping。 - done\n",
    "    4.降低learning rate。- done\n",
    "    5.两个LSTM?\n",
    "    \n",
    "    从实验的角度来看：\n",
    "        dense加入egularizer没什么用。\n",
    "        降低lr能小幅度提高。\n",
    "        两层dropout也没什么用。\n",
    "        early stopping只能节省时间。\n",
    "    \"\"\"\n",
    "    model_info_file_prefix = \"./model_info/{}_{}_\".format(model_name, current_time())\n",
    "    model_structure_file = model_info_file_prefix + \"model_structure.png\"\n",
    "    model_save_file = model_info_file_prefix + \"save_model\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(dict_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "    model.add(embedding_layer)\n",
    "    # 减少LSTM的个数\n",
    "    model.add(LSTM(16))\n",
    "    # 加入dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10,  kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    plot_model(model, to_file=model_structure_file, show_shapes=True)\n",
    "    \n",
    "    call_backs = generate_model_callbacks(model_save_file)\n",
    "    history_plot = LossHistory()\n",
    "    call_backs.append(history_plot)\n",
    "    \n",
    "    if isinstance(Y, pd.core.series.Series):\n",
    "        Y_train = Y_train.to_numpy()\n",
    "    if isinstance(Y_test, pd.core.series.Series):\n",
    "        Y_test = Y_test.to_numpy()\n",
    "    history = model.fit(X_train, Y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.2, callbacks=call_backs)\n",
    "    history_plot.loss_plot('epoch', model_info_file_prefix+\"epoch.png\")\n",
    "    epoch_performance(history, model_name)\n",
    "    score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"Test Score:\", score)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start training LSTM v4 model\")\n",
    "model7 = LSTM_many_to_one_model_v4(\"LSTM_v4\", vocab_size, embedding_mapping, maxlen, X_train, X_test, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
